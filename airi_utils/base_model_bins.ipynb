{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11d618c1-7bd7-4b54-b5c6-cd8fab531f12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_scatter import scatter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from DataClasses import lmdb_dataset, Dataset\n",
    "from ModelFunctions import train, evaluate, inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8705ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b083eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_reshape(tensor):\n",
    "    return torch.reshape(tensor, (tensor.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0099359-38ac-4375-965d-042045b5ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bins_torch(array_of_dfs):\n",
    "    thetas = []\n",
    "\n",
    "    for df in array_of_dfs:\n",
    "        theta = torch.tensor(df[:, 1])#.to('cpu')\n",
    "        theta = torch.histc(theta, bins=10, min=0, max=np.pi)\n",
    "        theta = torch.reshape(theta, (1, theta.shape[0]))\n",
    "        thetas.append(theta)\n",
    "        \n",
    "    thetas = torch.cat(thetas, 0)\n",
    "    \n",
    "    return thetas.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b82f4bae-3d55-42ec-a1b6-0b40219dac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_angles(array):\n",
    "    array[:, 1] = np.pi - array[:, 1]\n",
    "    array[:, 3] = - array[:, 3]\n",
    "    return array\n",
    "\n",
    "def restore_edge_angles(list_of_arrays):\n",
    "    el_new= []\n",
    "    for el in list_of_arrays:\n",
    "        el_new.append(el)\n",
    "        el_new.append(convert_angles(el.copy()))        \n",
    "    return el_new       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "643edca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#вызывается каждый раз, когда датасет отдаёт элемент (систему)\n",
    "#делаем из данных матрицу векторов-атомов, список рёбер (edge_index) и матрицу векторов-рёбер; надо писать свою функцию для каждой сети\n",
    "def preprocessing(system):\n",
    "    \n",
    "    tags = system['tags'].long()\n",
    "    tags = F.one_hot(tags, num_classes=3)\n",
    "    \n",
    "    atom_numbers = system['atomic_numbers'].long()\n",
    "    atom_numbers = F.one_hot(atom_numbers, num_classes=100)\n",
    "    \n",
    "    voronoi_volumes = system['voronoi_volumes'].float()\n",
    "    voronoi_volumes = my_reshape(voronoi_volumes)\n",
    "    \n",
    "    atom_features = (tags, atom_numbers, voronoi_volumes)#, spherical_radii)\n",
    "    atom_embeds = torch.cat(atom_features, 1)\n",
    "    \n",
    "    edge_index = system['edge_index_new'].long()\n",
    "    \n",
    "    distances = system['distances_new'].float()\n",
    "    distances = my_reshape(distances)\n",
    "    \n",
    "    \n",
    "    thetas = to_bins_torch(restore_edge_angles(system['edge_angles']))\n",
    "#     angles = system['contact_solid_angles'].float().to(device)\n",
    "#     angles = my_reshape(angles)\n",
    "\n",
    "    edge_features = (distances, thetas)\n",
    "    \n",
    "    edges_embeds = torch.cat(edge_features, 1)\n",
    "    \n",
    "    \n",
    "    return Data(x=atom_embeds.to(device), edge_index=edge_index.to(device), edge_attr=edges_embeds.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c8f958",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{x}_i^{(k)} = \\gamma^{(k)} \\left( \\mathbf{x}_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{j,i}\\right) \\right)\n",
    "$$\n",
    "\n",
    "Гамма лежит в апдейт, квадратик в aggr, а фи в месседж; в этом примере квадратик -- суммирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae848d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GConv(MessagePassing):\n",
    "    def __init__(self, dim_atom=104, dim_edge=11, out_channels=2):\n",
    "        super(GConv, self).__init__(aggr='add')  # \"Add\" aggregation\n",
    "        self.phi_output = 3\n",
    "        self.lin_phi = torch.nn.Linear(dim_atom*2+dim_edge, self.phi_output, bias=False)\n",
    "        self.lin_gamma = torch.nn.Linear(dim_atom + self.phi_output, out_channels, bias=False)\n",
    "        self.nonlin = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = batch['x']\n",
    "        edge_index = batch['edge_index']\n",
    "        edge_attr = batch['edge_attr']\n",
    "        \n",
    "        # x has shape [N -- количество атомов в системе(батче), in_channels -- размерность вектора-атома]\n",
    "        # edge_index has shape [2, E] -- каждое ребро задаётся парой вершин\n",
    "\n",
    "        # Start propagating messages. \n",
    "    \n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr, size=None)\n",
    "\n",
    "    def message(self, x, x_i, x_j, edge_attr):\n",
    "        concatenated = torch.cat((x_i, x_j, edge_attr), 1)\n",
    "        phi = self.lin_phi(concatenated)\n",
    "        phi = self.nonlin(phi)\n",
    "        return phi\n",
    "        \n",
    "    def update(self, aggr_out, x, edge_attr, edge_index):\n",
    "                \n",
    "        concatenated = torch.cat((x, aggr_out), 1)\n",
    "        gamma = self.lin_gamma(concatenated)\n",
    "        gamma = self.nonlin(gamma)\n",
    "\n",
    "        return Data(x=gamma, edge_attr=edge_attr, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e1232c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_atom=104, dim_edge=1):\n",
    "        \n",
    "        super().__init__()          \n",
    "        self.conv_last = GConv(dim_atom=dim_atom, dim_edge=dim_edge, out_channels=2)\n",
    "        \n",
    "        self.lin = torch.nn.Linear(2, 1, bias=True)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        convoluted_last = self.conv_last(batch)['x']\n",
    "        scattered = scatter(convoluted_last, batch['batch'], dim=0, reduce='sum')\n",
    "        summed = scattered\n",
    "        energy = self.lin(summed)\n",
    "        \n",
    "        return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "690ec366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "batch_size = 50\n",
    "num_workers = 0\n",
    "\n",
    "features_cols = ['atomic_numbers', 'edge_index_new', 'distances_new', \n",
    "                 'contact_solid_angles', 'tags', 'voronoi_volumes', 'edge_angles'] #он не нужен \n",
    "\n",
    "target_col = 'y_relaxed'\n",
    "lr = 0.001\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6e2796c-4bf3-4698-a99e-7059dd1f93d3",
   "metadata": {},
   "source": [
    "#чтобы тензор по умолчанию заводился на куде\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    print('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d2b308d-ffa4-4f48-adab-e218d7da0046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a58334d-3330-4cae-ae42-39f067667787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(edge_attr=[1214, 11], edge_index=[2, 1214], x=[86, 104]),\n",
       " -0.025550085000020317)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#инициализируем тренировочный датасети и тренировочный итератор\n",
    "train_dataset_file_path= os.path.expanduser(\"../../ocp_datasets/data/is2re/10k/train/data_mod2_compr.lmdb\")\n",
    "\n",
    "training_set = Dataset(train_dataset_file_path, features_cols, target_col, preprocessing=preprocessing)\n",
    "training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce747097-e5fd-4c36-b033-a8e6f2f122bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataLoader(training_set, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae852e60-b1ac-4a48-8120-af94ecec6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#инициализируем валидационный датасет и валидационный итератор\n",
    "val_dataset_file_path = os.path.expanduser(\"../../ocp_datasets/data/is2re/10k/train/data_mod2_compr.lmdb\")\n",
    "\n",
    "valid_set = Dataset(val_dataset_file_path, features_cols, target_col, preprocessing=preprocessing)\n",
    "valid_generator = DataLoader(valid_set, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f64a1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item: 0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    lmdb_dataset(train_dataset_file_path).describe()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f93e4f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "model = ConvNN(dim_atom=training_set[0][0]['x'].shape[1], dim_edge=training_set[0][0]['edge_attr'].shape[1])\n",
    "\n",
    "#optimizer and loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "#переносим на куду если она есть\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc1c0ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-14-20-40-01\n"
     ]
    }
   ],
   "source": [
    "timestamp = str(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a1cbede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard writer, при первом запуске надо руками сделать папку для логов\n",
    "\n",
    "# server\n",
    "#log_folder_path = \"../../ocp_results/logs/tensorboard/out_base_model\"\n",
    "\n",
    "# colab\n",
    "# log_folder_path = \"/content/drive/MyDrive/ocp_results/logs/tensorboard/out_base_model\"\n",
    "\n",
    "# user_specific \n",
    "log_file_path = os.path.expanduser(\"~/Documents/ocp_datasets_hd/logs/tensorboard_airi\")\n",
    "\n",
    "writer = SummaryWriter(log_file_path + '/' + timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e477b770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.23 s, sys: 43.6 ms, total: 1.28 s\n",
      "Wall time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logfile_str = {\n",
    "    \"train_dataset_file_path\": train_dataset_file_path,\n",
    "    \"val_dataset_file_path\": val_dataset_file_path,\n",
    "    \"features_cols\": features_cols,\n",
    "    \"target_col\": target_col,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"epochs\": epochs,\n",
    "    \"lr\": lr\n",
    "}\n",
    "\n",
    "#граф модели\n",
    "trace_system = dict(list(next(iter(training_generator))[0]))\n",
    "writer.add_graph(model, trace_system)\n",
    "writer.add_text(timestamp, str(logfile_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aca95f2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ca6c6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-14-20-40-01\n",
      "Start training model ConvNN(\n",
      "  (conv_last): GConv(\n",
      "    (lin_phi): Linear(in_features=219, out_features=3, bias=False)\n",
      "    (lin_gamma): Linear(in_features=107, out_features=2, bias=False)\n",
      "    (nonlin): Sigmoid()\n",
      "  )\n",
      "  (lin): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "epoch 0\n",
      "step 9 from 200 at epoch 0\n",
      "Loss: 58.36531066894531\n",
      "step 19 from 200 at epoch 0\n",
      "Loss: 51.932376861572266\n",
      "step 29 from 200 at epoch 0\n",
      "Loss: 42.915531158447266\n",
      "step 39 from 200 at epoch 0\n",
      "Loss: 34.65538024902344\n",
      "step 49 from 200 at epoch 0\n",
      "Loss: 25.3033447265625\n",
      "step 59 from 200 at epoch 0\n",
      "Loss: 16.861188888549805\n",
      "step 69 from 200 at epoch 0\n",
      "Loss: 7.72905969619751\n",
      "step 79 from 200 at epoch 0\n",
      "Loss: 4.666785717010498\n",
      "step 89 from 200 at epoch 0\n",
      "Loss: 4.1497344970703125\n",
      "step 99 from 200 at epoch 0\n",
      "Loss: 3.832829475402832\n",
      "step 109 from 200 at epoch 0\n",
      "Loss: 3.895029306411743\n",
      "step 119 from 200 at epoch 0\n",
      "Loss: 4.332376480102539\n",
      "step 129 from 200 at epoch 0\n",
      "Loss: 3.3011205196380615\n",
      "step 139 from 200 at epoch 0\n",
      "Loss: 3.592428207397461\n",
      "step 149 from 200 at epoch 0\n",
      "Loss: 3.269869327545166\n",
      "step 159 from 200 at epoch 0\n",
      "Loss: 3.374568462371826\n",
      "step 169 from 200 at epoch 0\n",
      "Loss: 3.1159605979919434\n",
      "step 179 from 200 at epoch 0\n",
      "Loss: 3.246177911758423\n",
      "step 189 from 200 at epoch 0\n",
      "Loss: 3.1853134632110596\n",
      "step 199 from 200 at epoch 0\n",
      "Loss: 3.488368034362793\n",
      "epoch 0 evaluation\n",
      "epoch loss 3.174641546010971\n",
      "========================================================================================================\n",
      "epoch 1\n",
      "step 9 from 200 at epoch 1\n",
      "Loss: 2.9199275970458984\n",
      "step 19 from 200 at epoch 1\n",
      "Loss: 3.040196180343628\n",
      "step 29 from 200 at epoch 1\n",
      "Loss: 2.9247779846191406\n",
      "step 39 from 200 at epoch 1\n",
      "Loss: 2.546654224395752\n",
      "step 49 from 200 at epoch 1\n",
      "Loss: 2.7785425186157227\n",
      "step 59 from 200 at epoch 1\n",
      "Loss: 3.1382832527160645\n",
      "step 69 from 200 at epoch 1\n",
      "Loss: 3.404158353805542\n",
      "step 79 from 200 at epoch 1\n",
      "Loss: 2.5731565952301025\n",
      "step 89 from 200 at epoch 1\n",
      "Loss: 2.8054826259613037\n",
      "step 99 from 200 at epoch 1\n",
      "Loss: 2.812753915786743\n",
      "step 109 from 200 at epoch 1\n",
      "Loss: 2.999392032623291\n",
      "step 119 from 200 at epoch 1\n",
      "Loss: 3.1438581943511963\n",
      "step 129 from 200 at epoch 1\n",
      "Loss: 2.5182394981384277\n",
      "step 139 from 200 at epoch 1\n",
      "Loss: 2.8916454315185547\n",
      "step 149 from 200 at epoch 1\n",
      "Loss: 2.689239978790283\n",
      "step 159 from 200 at epoch 1\n",
      "Loss: 2.877171277999878\n",
      "step 169 from 200 at epoch 1\n",
      "Loss: 2.543435573577881\n",
      "step 179 from 200 at epoch 1\n",
      "Loss: 2.7084035873413086\n",
      "step 189 from 200 at epoch 1\n",
      "Loss: 2.6134228706359863\n",
      "step 199 from 200 at epoch 1\n",
      "Loss: 3.0625362396240234\n",
      "epoch 1 evaluation\n",
      "epoch loss 2.694633364677429\n",
      "========================================================================================================\n",
      "epoch 2\n",
      "step 9 from 200 at epoch 2\n",
      "Loss: 2.507688283920288\n",
      "step 19 from 200 at epoch 2\n",
      "Loss: 2.629462242126465\n",
      "step 29 from 200 at epoch 2\n",
      "Loss: 2.5170419216156006\n",
      "step 39 from 200 at epoch 2\n",
      "Loss: 2.2622900009155273\n",
      "step 49 from 200 at epoch 2\n",
      "Loss: 2.420060396194458\n",
      "step 59 from 200 at epoch 2\n",
      "Loss: 2.7948946952819824\n",
      "step 69 from 200 at epoch 2\n",
      "Loss: 3.068512201309204\n",
      "step 79 from 200 at epoch 2\n",
      "Loss: 2.231154680252075\n",
      "step 89 from 200 at epoch 2\n",
      "Loss: 2.5068936347961426\n",
      "step 99 from 200 at epoch 2\n",
      "Loss: 2.556375741958618\n",
      "step 109 from 200 at epoch 2\n",
      "Loss: 2.7354817390441895\n",
      "step 119 from 200 at epoch 2\n",
      "Loss: 2.735452890396118\n",
      "step 129 from 200 at epoch 2\n",
      "Loss: 2.226393938064575\n",
      "step 139 from 200 at epoch 2\n",
      "Loss: 2.627446174621582\n",
      "step 149 from 200 at epoch 2\n",
      "Loss: 2.4538931846618652\n",
      "step 159 from 200 at epoch 2\n",
      "Loss: 2.6652331352233887\n",
      "step 169 from 200 at epoch 2\n",
      "Loss: 2.2916600704193115\n",
      "step 179 from 200 at epoch 2\n",
      "Loss: 2.45452880859375\n",
      "step 189 from 200 at epoch 2\n",
      "Loss: 2.354243040084839\n",
      "step 199 from 200 at epoch 2\n",
      "Loss: 2.8528294563293457\n",
      "epoch 2 evaluation\n",
      "epoch loss 2.4723910337686537\n",
      "========================================================================================================\n",
      "epoch 3\n",
      "step 9 from 200 at epoch 3\n",
      "Loss: 2.307518720626831\n",
      "step 19 from 200 at epoch 3\n",
      "Loss: 2.434925079345703\n",
      "step 29 from 200 at epoch 3\n",
      "Loss: 2.304745674133301\n",
      "step 39 from 200 at epoch 3\n",
      "Loss: 2.1097657680511475\n",
      "step 49 from 200 at epoch 3\n",
      "Loss: 2.224567174911499\n",
      "step 59 from 200 at epoch 3\n",
      "Loss: 2.6136631965637207\n",
      "step 69 from 200 at epoch 3\n",
      "Loss: 2.8780763149261475\n",
      "step 79 from 200 at epoch 3\n",
      "Loss: 2.0541460514068604\n",
      "step 89 from 200 at epoch 3\n",
      "Loss: 2.3416879177093506\n",
      "step 99 from 200 at epoch 3\n",
      "Loss: 2.411386251449585\n",
      "step 109 from 200 at epoch 3\n",
      "Loss: 2.5591652393341064\n",
      "step 119 from 200 at epoch 3\n",
      "Loss: 2.4895498752593994\n",
      "step 129 from 200 at epoch 3\n",
      "Loss: 2.034853219985962\n",
      "step 139 from 200 at epoch 3\n",
      "Loss: 2.4507994651794434\n",
      "step 149 from 200 at epoch 3\n",
      "Loss: 2.305421829223633\n",
      "step 159 from 200 at epoch 3\n",
      "Loss: 2.513413906097412\n",
      "step 169 from 200 at epoch 3\n",
      "Loss: 2.141021728515625\n",
      "step 179 from 200 at epoch 3\n",
      "Loss: 2.2983975410461426\n",
      "step 189 from 200 at epoch 3\n",
      "Loss: 2.177567481994629\n",
      "step 199 from 200 at epoch 3\n",
      "Loss: 2.709684371948242\n",
      "epoch 3 evaluation\n",
      "epoch loss 2.318801379799843\n",
      "========================================================================================================\n",
      "epoch 4\n",
      "step 9 from 200 at epoch 4\n",
      "Loss: 2.1662144660949707\n",
      "step 19 from 200 at epoch 4\n",
      "Loss: 2.30102276802063\n",
      "step 29 from 200 at epoch 4\n",
      "Loss: 2.161794424057007\n",
      "step 39 from 200 at epoch 4\n",
      "Loss: 2.00077223777771\n",
      "step 49 from 200 at epoch 4\n",
      "Loss: 2.0777809619903564\n",
      "step 59 from 200 at epoch 4\n",
      "Loss: 2.472278118133545\n",
      "step 69 from 200 at epoch 4\n",
      "Loss: 2.743405342102051\n",
      "step 79 from 200 at epoch 4\n",
      "Loss: 1.921700119972229\n",
      "step 89 from 200 at epoch 4\n",
      "Loss: 2.213953971862793\n",
      "step 99 from 200 at epoch 4\n",
      "Loss: 2.311631202697754\n",
      "step 109 from 200 at epoch 4\n",
      "Loss: 2.4188501834869385\n",
      "step 119 from 200 at epoch 4\n",
      "Loss: 2.3150222301483154\n",
      "step 129 from 200 at epoch 4\n",
      "Loss: 1.8869317770004272\n",
      "step 139 from 200 at epoch 4\n",
      "Loss: 2.3177592754364014\n",
      "step 149 from 200 at epoch 4\n",
      "Loss: 2.1900885105133057\n",
      "step 159 from 200 at epoch 4\n",
      "Loss: 2.393465042114258\n",
      "step 169 from 200 at epoch 4\n",
      "Loss: 2.023538827896118\n",
      "step 179 from 200 at epoch 4\n",
      "Loss: 2.1940155029296875\n",
      "step 189 from 200 at epoch 4\n",
      "Loss: 2.035724401473999\n",
      "step 199 from 200 at epoch 4\n",
      "Loss: 2.6007416248321533\n",
      "epoch 4 evaluation\n",
      "epoch loss 2.198774676322937\n",
      "========================================================================================================\n",
      "epoch 5\n",
      "step 9 from 200 at epoch 5\n",
      "Loss: 2.064565658569336\n",
      "step 19 from 200 at epoch 5\n",
      "Loss: 2.194451332092285\n",
      "step 29 from 200 at epoch 5\n",
      "Loss: 2.0483920574188232\n",
      "step 39 from 200 at epoch 5\n",
      "Loss: 1.9129630327224731\n",
      "step 49 from 200 at epoch 5\n",
      "Loss: 1.9618908166885376\n",
      "step 59 from 200 at epoch 5\n",
      "Loss: 2.351365327835083\n",
      "step 69 from 200 at epoch 5\n",
      "Loss: 2.6336162090301514\n",
      "step 79 from 200 at epoch 5\n",
      "Loss: 1.8135918378829956\n",
      "step 89 from 200 at epoch 5\n",
      "Loss: 2.1011054515838623\n",
      "step 99 from 200 at epoch 5\n",
      "Loss: 2.243164539337158\n",
      "step 109 from 200 at epoch 5\n",
      "Loss: 2.291623592376709\n",
      "step 119 from 200 at epoch 5\n",
      "Loss: 2.1804864406585693\n",
      "step 129 from 200 at epoch 5\n",
      "Loss: 1.7694354057312012\n",
      "step 139 from 200 at epoch 5\n",
      "Loss: 2.2014033794403076\n",
      "step 149 from 200 at epoch 5\n",
      "Loss: 2.097910165786743\n",
      "step 159 from 200 at epoch 5\n",
      "Loss: 2.289942979812622\n",
      "step 169 from 200 at epoch 5\n",
      "Loss: 1.9288705587387085\n",
      "step 179 from 200 at epoch 5\n",
      "Loss: 2.1051437854766846\n",
      "step 189 from 200 at epoch 5\n",
      "Loss: 1.9005568027496338\n",
      "step 199 from 200 at epoch 5\n",
      "Loss: 2.509493350982666\n",
      "epoch 5 evaluation\n",
      "epoch loss 2.092705952525139\n",
      "========================================================================================================\n",
      "epoch 6\n",
      "step 9 from 200 at epoch 6\n",
      "Loss: 1.9770723581314087\n",
      "step 19 from 200 at epoch 6\n",
      "Loss: 2.1002724170684814\n",
      "step 29 from 200 at epoch 6\n",
      "Loss: 1.944693922996521\n",
      "step 39 from 200 at epoch 6\n",
      "Loss: 1.835936188697815\n",
      "step 49 from 200 at epoch 6\n",
      "Loss: 1.8724504709243774\n",
      "step 59 from 200 at epoch 6\n",
      "Loss: 2.2437875270843506\n",
      "step 69 from 200 at epoch 6\n",
      "Loss: 2.527026414871216\n",
      "step 79 from 200 at epoch 6\n",
      "Loss: 1.7190157175064087\n",
      "step 89 from 200 at epoch 6\n",
      "Loss: 1.9947799444198608\n",
      "step 99 from 200 at epoch 6\n",
      "Loss: 2.193446397781372\n",
      "step 109 from 200 at epoch 6\n",
      "Loss: 2.178280830383301\n",
      "step 119 from 200 at epoch 6\n",
      "Loss: 2.0545895099639893\n",
      "step 129 from 200 at epoch 6\n",
      "Loss: 1.681148648262024\n",
      "step 139 from 200 at epoch 6\n",
      "Loss: 2.119412422180176\n",
      "step 149 from 200 at epoch 6\n",
      "Loss: 2.0260186195373535\n",
      "step 159 from 200 at epoch 6\n",
      "Loss: 2.209275960922241\n",
      "step 169 from 200 at epoch 6\n",
      "Loss: 1.8521969318389893\n",
      "step 179 from 200 at epoch 6\n",
      "Loss: 2.03236722946167\n",
      "step 189 from 200 at epoch 6\n",
      "Loss: 1.7933868169784546\n",
      "step 199 from 200 at epoch 6\n",
      "Loss: 2.4345414638519287\n",
      "epoch 6 evaluation\n",
      "epoch loss 2.013015460371971\n",
      "========================================================================================================\n",
      "epoch 7\n",
      "step 9 from 200 at epoch 7\n",
      "Loss: 1.9191007614135742\n",
      "step 19 from 200 at epoch 7\n",
      "Loss: 2.0327460765838623\n",
      "step 29 from 200 at epoch 7\n",
      "Loss: 1.8672579526901245\n",
      "step 39 from 200 at epoch 7\n",
      "Loss: 1.7756154537200928\n",
      "step 49 from 200 at epoch 7\n",
      "Loss: 1.8000458478927612\n",
      "step 59 from 200 at epoch 7\n",
      "Loss: 2.166652202606201\n",
      "step 69 from 200 at epoch 7\n",
      "Loss: 2.4460201263427734\n",
      "step 79 from 200 at epoch 7\n",
      "Loss: 1.6458075046539307\n",
      "step 89 from 200 at epoch 7\n",
      "Loss: 1.9179890155792236\n",
      "step 99 from 200 at epoch 7\n",
      "Loss: 2.162280797958374\n",
      "step 109 from 200 at epoch 7\n",
      "Loss: 2.087500810623169\n",
      "step 119 from 200 at epoch 7\n",
      "Loss: 1.9583911895751953\n",
      "step 129 from 200 at epoch 7\n",
      "Loss: 1.6274343729019165\n",
      "step 139 from 200 at epoch 7\n",
      "Loss: 2.066187620162964\n",
      "step 149 from 200 at epoch 7\n",
      "Loss: 1.982909917831421\n",
      "step 159 from 200 at epoch 7\n",
      "Loss: 2.148538112640381\n",
      "step 169 from 200 at epoch 7\n",
      "Loss: 1.791670799255371\n",
      "step 179 from 200 at epoch 7\n",
      "Loss: 1.9727263450622559\n",
      "step 189 from 200 at epoch 7\n",
      "Loss: 1.7161006927490234\n",
      "step 199 from 200 at epoch 7\n",
      "Loss: 2.3749585151672363\n",
      "epoch 7 evaluation\n",
      "epoch loss 1.9527450251579284\n",
      "========================================================================================================\n",
      "epoch 8\n",
      "step 9 from 200 at epoch 8\n",
      "Loss: 1.873382568359375\n",
      "step 19 from 200 at epoch 8\n",
      "Loss: 1.9882433414459229\n",
      "step 29 from 200 at epoch 8\n",
      "Loss: 1.8097862005233765\n",
      "step 39 from 200 at epoch 8\n",
      "Loss: 1.7228959798812866\n",
      "step 49 from 200 at epoch 8\n",
      "Loss: 1.743330955505371\n",
      "step 59 from 200 at epoch 8\n",
      "Loss: 2.1020734310150146\n",
      "step 69 from 200 at epoch 8\n",
      "Loss: 2.382122039794922\n",
      "step 79 from 200 at epoch 8\n",
      "Loss: 1.5924248695373535\n",
      "step 89 from 200 at epoch 8\n",
      "Loss: 1.859830617904663\n",
      "step 99 from 200 at epoch 8\n",
      "Loss: 2.1371612548828125\n",
      "step 109 from 200 at epoch 8\n",
      "Loss: 2.0142126083374023\n",
      "step 119 from 200 at epoch 8\n",
      "Loss: 1.8775094747543335\n",
      "step 129 from 200 at epoch 8\n",
      "Loss: 1.595570683479309\n",
      "step 139 from 200 at epoch 8\n",
      "Loss: 2.0268003940582275\n",
      "step 149 from 200 at epoch 8\n",
      "Loss: 1.9486637115478516\n",
      "step 159 from 200 at epoch 8\n",
      "Loss: 2.093679428100586\n",
      "step 169 from 200 at epoch 8\n",
      "Loss: 1.7450988292694092\n",
      "step 179 from 200 at epoch 8\n",
      "Loss: 1.922455072402954\n",
      "step 189 from 200 at epoch 8\n",
      "Loss: 1.6618895530700684\n",
      "step 199 from 200 at epoch 8\n",
      "Loss: 2.327714681625366\n",
      "epoch 8 evaluation\n",
      "epoch loss 1.9040433079004289\n",
      "========================================================================================================\n",
      "epoch 9\n",
      "step 9 from 200 at epoch 9\n",
      "Loss: 1.843709111213684\n",
      "step 19 from 200 at epoch 9\n",
      "Loss: 1.9494614601135254\n",
      "step 29 from 200 at epoch 9\n",
      "Loss: 1.7627661228179932\n",
      "step 39 from 200 at epoch 9\n",
      "Loss: 1.6774080991744995\n",
      "step 49 from 200 at epoch 9\n",
      "Loss: 1.696164846420288\n",
      "step 59 from 200 at epoch 9\n",
      "Loss: 2.0496950149536133\n",
      "step 69 from 200 at epoch 9\n",
      "Loss: 2.3301808834075928\n",
      "step 79 from 200 at epoch 9\n",
      "Loss: 1.5581387281417847\n",
      "step 89 from 200 at epoch 9\n",
      "Loss: 1.8108264207839966\n",
      "step 99 from 200 at epoch 9\n",
      "Loss: 2.1146011352539062\n",
      "step 109 from 200 at epoch 9\n",
      "Loss: 1.947218894958496\n",
      "step 119 from 200 at epoch 9\n",
      "Loss: 1.808061957359314\n",
      "step 129 from 200 at epoch 9\n",
      "Loss: 1.572084903717041\n",
      "step 139 from 200 at epoch 9\n",
      "Loss: 1.9946646690368652\n",
      "step 149 from 200 at epoch 9\n",
      "Loss: 1.9200003147125244\n",
      "step 159 from 200 at epoch 9\n",
      "Loss: 2.044778347015381\n",
      "step 169 from 200 at epoch 9\n",
      "Loss: 1.7107946872711182\n",
      "step 179 from 200 at epoch 9\n",
      "Loss: 1.8830479383468628\n",
      "step 189 from 200 at epoch 9\n",
      "Loss: 1.6275821924209595\n",
      "step 199 from 200 at epoch 9\n",
      "Loss: 2.287733793258667\n",
      "epoch 9 evaluation\n",
      "epoch loss 1.8649960738420486\n",
      "========================================================================================================\n",
      "epoch 10\n",
      "step 9 from 200 at epoch 10\n",
      "Loss: 1.822312593460083\n",
      "step 19 from 200 at epoch 10\n",
      "Loss: 1.9203815460205078\n",
      "step 29 from 200 at epoch 10\n",
      "Loss: 1.727892279624939\n",
      "step 39 from 200 at epoch 10\n",
      "Loss: 1.6476125717163086\n",
      "step 49 from 200 at epoch 10\n",
      "Loss: 1.6612355709075928\n",
      "step 59 from 200 at epoch 10\n",
      "Loss: 2.007789373397827\n",
      "step 69 from 200 at epoch 10\n",
      "Loss: 2.286041498184204\n",
      "step 79 from 200 at epoch 10\n",
      "Loss: 1.538686990737915\n",
      "step 89 from 200 at epoch 10\n",
      "Loss: 1.7670516967773438\n",
      "step 99 from 200 at epoch 10\n",
      "Loss: 2.09602427482605\n",
      "step 109 from 200 at epoch 10\n",
      "Loss: 1.8976452350616455\n",
      "step 119 from 200 at epoch 10\n",
      "Loss: 1.7448474168777466\n",
      "step 129 from 200 at epoch 10\n",
      "Loss: 1.5547455549240112\n",
      "step 139 from 200 at epoch 10\n",
      "Loss: 1.9682928323745728\n",
      "step 149 from 200 at epoch 10\n",
      "Loss: 1.9008398056030273\n",
      "step 159 from 200 at epoch 10\n",
      "Loss: 2.0029284954071045\n",
      "step 169 from 200 at epoch 10\n",
      "Loss: 1.6928988695144653\n",
      "step 179 from 200 at epoch 10\n",
      "Loss: 1.8560115098953247\n",
      "step 189 from 200 at epoch 10\n",
      "Loss: 1.6038488149642944\n",
      "step 199 from 200 at epoch 10\n",
      "Loss: 2.251189708709717\n",
      "epoch 10 evaluation\n",
      "epoch loss 1.8343195539712907\n",
      "========================================================================================================\n",
      "epoch 11\n",
      "step 9 from 200 at epoch 11\n",
      "Loss: 1.8100615739822388\n",
      "step 19 from 200 at epoch 11\n",
      "Loss: 1.9011837244033813\n",
      "step 29 from 200 at epoch 11\n",
      "Loss: 1.700319528579712\n",
      "step 39 from 200 at epoch 11\n",
      "Loss: 1.6249463558197021\n",
      "step 49 from 200 at epoch 11\n",
      "Loss: 1.650856852531433\n",
      "step 59 from 200 at epoch 11\n",
      "Loss: 1.9795461893081665\n",
      "step 69 from 200 at epoch 11\n",
      "Loss: 2.2460274696350098\n",
      "step 79 from 200 at epoch 11\n",
      "Loss: 1.5269722938537598\n",
      "step 89 from 200 at epoch 11\n",
      "Loss: 1.7333693504333496\n",
      "step 99 from 200 at epoch 11\n",
      "Loss: 2.0820775032043457\n",
      "step 109 from 200 at epoch 11\n",
      "Loss: 1.858160376548767\n",
      "step 119 from 200 at epoch 11\n",
      "Loss: 1.6884008646011353\n",
      "step 129 from 200 at epoch 11\n",
      "Loss: 1.5420199632644653\n",
      "step 139 from 200 at epoch 11\n",
      "Loss: 1.9468926191329956\n",
      "step 149 from 200 at epoch 11\n",
      "Loss: 1.8858044147491455\n",
      "step 159 from 200 at epoch 11\n",
      "Loss: 1.9699971675872803\n",
      "step 169 from 200 at epoch 11\n",
      "Loss: 1.6801131963729858\n",
      "step 179 from 200 at epoch 11\n",
      "Loss: 1.8425394296646118\n",
      "step 189 from 200 at epoch 11\n",
      "Loss: 1.582680583000183\n",
      "step 199 from 200 at epoch 11\n",
      "Loss: 2.22214674949646\n",
      "epoch 11 evaluation\n",
      "epoch loss 1.8103440082073212\n",
      "========================================================================================================\n",
      "epoch 12\n",
      "step 9 from 200 at epoch 12\n",
      "Loss: 1.8056371212005615\n",
      "step 19 from 200 at epoch 12\n",
      "Loss: 1.8870806694030762\n",
      "step 29 from 200 at epoch 12\n",
      "Loss: 1.677122712135315\n",
      "step 39 from 200 at epoch 12\n",
      "Loss: 1.6043497323989868\n",
      "step 49 from 200 at epoch 12\n",
      "Loss: 1.6632674932479858\n",
      "step 59 from 200 at epoch 12\n",
      "Loss: 1.9551421403884888\n",
      "step 69 from 200 at epoch 12\n",
      "Loss: 2.2109336853027344\n",
      "step 79 from 200 at epoch 12\n",
      "Loss: 1.519177794456482\n",
      "step 89 from 200 at epoch 12\n",
      "Loss: 1.7052041292190552\n",
      "step 99 from 200 at epoch 12\n",
      "Loss: 2.0694475173950195\n",
      "step 109 from 200 at epoch 12\n",
      "Loss: 1.8226604461669922\n",
      "step 119 from 200 at epoch 12\n",
      "Loss: 1.640411376953125\n",
      "step 129 from 200 at epoch 12\n",
      "Loss: 1.5323939323425293\n",
      "step 139 from 200 at epoch 12\n",
      "Loss: 1.927341103553772\n",
      "step 149 from 200 at epoch 12\n",
      "Loss: 1.8758615255355835\n",
      "step 159 from 200 at epoch 12\n",
      "Loss: 1.9441951513290405\n",
      "step 169 from 200 at epoch 12\n",
      "Loss: 1.668422818183899\n",
      "step 179 from 200 at epoch 12\n",
      "Loss: 1.833902359008789\n",
      "step 189 from 200 at epoch 12\n",
      "Loss: 1.566914439201355\n",
      "step 199 from 200 at epoch 12\n",
      "Loss: 2.19917368888855\n",
      "epoch 12 evaluation\n",
      "epoch loss 1.791357505917549\n",
      "========================================================================================================\n",
      "epoch 13\n",
      "step 9 from 200 at epoch 13\n",
      "Loss: 1.809324026107788\n",
      "step 19 from 200 at epoch 13\n",
      "Loss: 1.8767216205596924\n",
      "step 29 from 200 at epoch 13\n",
      "Loss: 1.6604219675064087\n",
      "step 39 from 200 at epoch 13\n",
      "Loss: 1.5875442028045654\n",
      "step 49 from 200 at epoch 13\n",
      "Loss: 1.677753210067749\n",
      "step 59 from 200 at epoch 13\n",
      "Loss: 1.933890700340271\n",
      "step 69 from 200 at epoch 13\n",
      "Loss: 2.1813766956329346\n",
      "step 79 from 200 at epoch 13\n",
      "Loss: 1.5144338607788086\n",
      "step 89 from 200 at epoch 13\n",
      "Loss: 1.679552674293518\n",
      "step 99 from 200 at epoch 13\n",
      "Loss: 2.058013439178467\n",
      "step 109 from 200 at epoch 13\n",
      "Loss: 1.790336012840271\n",
      "step 119 from 200 at epoch 13\n",
      "Loss: 1.5968279838562012\n",
      "step 129 from 200 at epoch 13\n",
      "Loss: 1.5322822332382202\n",
      "step 139 from 200 at epoch 13\n",
      "Loss: 1.911138892173767\n",
      "step 149 from 200 at epoch 13\n",
      "Loss: 1.8748600482940674\n",
      "step 159 from 200 at epoch 13\n",
      "Loss: 1.9265612363815308\n",
      "step 169 from 200 at epoch 13\n",
      "Loss: 1.6589099168777466\n",
      "step 179 from 200 at epoch 13\n",
      "Loss: 1.8260886669158936\n",
      "step 189 from 200 at epoch 13\n",
      "Loss: 1.5561246871948242\n",
      "step 199 from 200 at epoch 13\n",
      "Loss: 2.18050217628479\n",
      "epoch 13 evaluation\n",
      "epoch loss 1.7768561911582947\n",
      "========================================================================================================\n",
      "epoch 14\n",
      "step 9 from 200 at epoch 14\n",
      "Loss: 1.812658667564392\n",
      "step 19 from 200 at epoch 14\n",
      "Loss: 1.869484305381775\n",
      "step 29 from 200 at epoch 14\n",
      "Loss: 1.6488234996795654\n",
      "step 39 from 200 at epoch 14\n",
      "Loss: 1.5772844552993774\n",
      "step 49 from 200 at epoch 14\n",
      "Loss: 1.692773699760437\n",
      "step 59 from 200 at epoch 14\n",
      "Loss: 1.9203563928604126\n",
      "step 69 from 200 at epoch 14\n",
      "Loss: 2.1545660495758057\n",
      "step 79 from 200 at epoch 14\n",
      "Loss: 1.517863392829895\n",
      "step 89 from 200 at epoch 14\n",
      "Loss: 1.656116247177124\n",
      "step 99 from 200 at epoch 14\n",
      "Loss: 2.0478670597076416\n",
      "step 109 from 200 at epoch 14\n",
      "Loss: 1.7607711553573608\n",
      "step 119 from 200 at epoch 14\n",
      "Loss: 1.5638833045959473\n",
      "step 129 from 200 at epoch 14\n",
      "Loss: 1.5355263948440552\n",
      "step 139 from 200 at epoch 14\n",
      "Loss: 1.8991143703460693\n",
      "step 149 from 200 at epoch 14\n",
      "Loss: 1.8746083974838257\n",
      "step 159 from 200 at epoch 14\n",
      "Loss: 1.9123355150222778\n",
      "step 169 from 200 at epoch 14\n",
      "Loss: 1.6524828672409058\n",
      "step 179 from 200 at epoch 14\n",
      "Loss: 1.8191841840744019\n",
      "step 189 from 200 at epoch 14\n",
      "Loss: 1.5520458221435547\n",
      "step 199 from 200 at epoch 14\n",
      "Loss: 2.1659088134765625\n",
      "epoch 14 evaluation\n",
      "epoch loss 1.7660309231281281\n",
      "========================================================================================================\n",
      "epoch 15\n",
      "step 9 from 200 at epoch 15\n",
      "Loss: 1.8156145811080933\n",
      "step 19 from 200 at epoch 15\n",
      "Loss: 1.8645131587982178\n",
      "step 29 from 200 at epoch 15\n",
      "Loss: 1.6410787105560303\n",
      "step 39 from 200 at epoch 15\n",
      "Loss: 1.5693279504776\n",
      "step 49 from 200 at epoch 15\n",
      "Loss: 1.7074745893478394\n",
      "step 59 from 200 at epoch 15\n",
      "Loss: 1.9084383249282837\n",
      "step 69 from 200 at epoch 15\n",
      "Loss: 2.1337027549743652\n",
      "step 79 from 200 at epoch 15\n",
      "Loss: 1.5208512544631958\n",
      "step 89 from 200 at epoch 15\n",
      "Loss: 1.6357359886169434\n",
      "step 99 from 200 at epoch 15\n",
      "Loss: 2.0416557788848877\n",
      "step 109 from 200 at epoch 15\n",
      "Loss: 1.7349714040756226\n",
      "step 119 from 200 at epoch 15\n",
      "Loss: 1.5358065366744995\n",
      "step 129 from 200 at epoch 15\n",
      "Loss: 1.5410609245300293\n",
      "step 139 from 200 at epoch 15\n",
      "Loss: 1.8934170007705688\n",
      "step 149 from 200 at epoch 15\n",
      "Loss: 1.8744022846221924\n",
      "step 159 from 200 at epoch 15\n",
      "Loss: 1.9034740924835205\n",
      "step 169 from 200 at epoch 15\n",
      "Loss: 1.6480202674865723\n",
      "step 179 from 200 at epoch 15\n",
      "Loss: 1.8132623434066772\n",
      "step 189 from 200 at epoch 15\n",
      "Loss: 1.5496292114257812\n",
      "step 199 from 200 at epoch 15\n",
      "Loss: 2.155385732650757\n",
      "epoch 15 evaluation\n",
      "epoch loss 1.758665006160736\n",
      "========================================================================================================\n",
      "epoch 16\n",
      "step 9 from 200 at epoch 16\n",
      "Loss: 1.8186269998550415\n",
      "step 19 from 200 at epoch 16\n",
      "Loss: 1.8616629838943481\n",
      "step 29 from 200 at epoch 16\n",
      "Loss: 1.6362196207046509\n",
      "step 39 from 200 at epoch 16\n",
      "Loss: 1.5660765171051025\n",
      "step 49 from 200 at epoch 16\n",
      "Loss: 1.7220138311386108\n",
      "step 59 from 200 at epoch 16\n",
      "Loss: 1.8986375331878662\n",
      "step 69 from 200 at epoch 16\n",
      "Loss: 2.1167831420898438\n",
      "step 79 from 200 at epoch 16\n",
      "Loss: 1.5233031511306763\n",
      "step 89 from 200 at epoch 16\n",
      "Loss: 1.6189581155776978\n",
      "step 99 from 200 at epoch 16\n",
      "Loss: 2.036558151245117\n",
      "step 109 from 200 at epoch 16\n",
      "Loss: 1.7139244079589844\n",
      "step 119 from 200 at epoch 16\n",
      "Loss: 1.5135329961776733\n",
      "step 129 from 200 at epoch 16\n",
      "Loss: 1.5468719005584717\n",
      "step 139 from 200 at epoch 16\n",
      "Loss: 1.889151930809021\n",
      "step 149 from 200 at epoch 16\n",
      "Loss: 1.874237060546875\n",
      "step 159 from 200 at epoch 16\n",
      "Loss: 1.8967057466506958\n",
      "step 169 from 200 at epoch 16\n",
      "Loss: 1.6460610628128052\n",
      "step 179 from 200 at epoch 16\n",
      "Loss: 1.808356761932373\n",
      "step 189 from 200 at epoch 16\n",
      "Loss: 1.5494898557662964\n",
      "step 199 from 200 at epoch 16\n",
      "Loss: 2.146592855453491\n",
      "epoch 16 evaluation\n",
      "epoch loss 1.7538097470998764\n",
      "========================================================================================================\n",
      "epoch 17\n",
      "step 9 from 200 at epoch 17\n",
      "Loss: 1.822934627532959\n",
      "step 19 from 200 at epoch 17\n",
      "Loss: 1.861039638519287\n",
      "step 29 from 200 at epoch 17\n",
      "Loss: 1.6343563795089722\n",
      "step 39 from 200 at epoch 17\n",
      "Loss: 1.5639382600784302\n",
      "step 49 from 200 at epoch 17\n",
      "Loss: 1.7363042831420898\n",
      "step 59 from 200 at epoch 17\n",
      "Loss: 1.890335202217102\n",
      "step 69 from 200 at epoch 17\n",
      "Loss: 2.1040544509887695\n",
      "step 79 from 200 at epoch 17\n",
      "Loss: 1.5253679752349854\n",
      "step 89 from 200 at epoch 17\n",
      "Loss: 1.6077160835266113\n",
      "step 99 from 200 at epoch 17\n",
      "Loss: 2.0340664386749268\n",
      "step 109 from 200 at epoch 17\n",
      "Loss: 1.6965479850769043\n",
      "step 119 from 200 at epoch 17\n",
      "Loss: 1.4966692924499512\n",
      "step 129 from 200 at epoch 17\n",
      "Loss: 1.552554726600647\n",
      "step 139 from 200 at epoch 17\n",
      "Loss: 1.8886774778366089\n",
      "step 149 from 200 at epoch 17\n",
      "Loss: 1.8740997314453125\n",
      "step 159 from 200 at epoch 17\n",
      "Loss: 1.8912240266799927\n",
      "step 169 from 200 at epoch 17\n",
      "Loss: 1.6470081806182861\n",
      "step 179 from 200 at epoch 17\n",
      "Loss: 1.8068389892578125\n",
      "step 189 from 200 at epoch 17\n",
      "Loss: 1.5493731498718262\n",
      "step 199 from 200 at epoch 17\n",
      "Loss: 2.1394972801208496\n",
      "epoch 17 evaluation\n",
      "epoch loss 1.7508476376533508\n",
      "========================================================================================================\n",
      "epoch 18\n",
      "step 9 from 200 at epoch 18\n",
      "Loss: 1.8281904458999634\n",
      "step 19 from 200 at epoch 18\n",
      "Loss: 1.8631843328475952\n",
      "step 29 from 200 at epoch 18\n",
      "Loss: 1.6343129873275757\n",
      "step 39 from 200 at epoch 18\n",
      "Loss: 1.5621914863586426\n",
      "step 49 from 200 at epoch 18\n",
      "Loss: 1.7481762170791626\n",
      "step 59 from 200 at epoch 18\n",
      "Loss: 1.8846981525421143\n",
      "step 69 from 200 at epoch 18\n",
      "Loss: 2.0939929485321045\n",
      "step 79 from 200 at epoch 18\n",
      "Loss: 1.5270360708236694\n",
      "step 89 from 200 at epoch 18\n",
      "Loss: 1.599805474281311\n",
      "step 99 from 200 at epoch 18\n",
      "Loss: 2.0322089195251465\n",
      "step 109 from 200 at epoch 18\n",
      "Loss: 1.684121012687683\n",
      "step 119 from 200 at epoch 18\n",
      "Loss: 1.4831959009170532\n",
      "step 129 from 200 at epoch 18\n",
      "Loss: 1.5584945678710938\n",
      "step 139 from 200 at epoch 18\n",
      "Loss: 1.8886120319366455\n",
      "step 149 from 200 at epoch 18\n",
      "Loss: 1.873980164527893\n",
      "step 159 from 200 at epoch 18\n",
      "Loss: 1.8870644569396973\n",
      "step 169 from 200 at epoch 18\n",
      "Loss: 1.64956533908844\n",
      "step 179 from 200 at epoch 18\n",
      "Loss: 1.8065848350524902\n",
      "step 189 from 200 at epoch 18\n",
      "Loss: 1.5492708683013916\n",
      "step 199 from 200 at epoch 18\n",
      "Loss: 2.134047508239746\n",
      "epoch 18 evaluation\n",
      "epoch loss 1.7491472041606904\n",
      "========================================================================================================\n",
      "epoch 19\n",
      "step 9 from 200 at epoch 19\n",
      "Loss: 1.832163691520691\n",
      "step 19 from 200 at epoch 19\n",
      "Loss: 1.865692138671875\n",
      "step 29 from 200 at epoch 19\n",
      "Loss: 1.636305332183838\n",
      "step 39 from 200 at epoch 19\n",
      "Loss: 1.5621153116226196\n",
      "step 49 from 200 at epoch 19\n",
      "Loss: 1.7567715644836426\n",
      "step 59 from 200 at epoch 19\n",
      "Loss: 1.882193922996521\n",
      "step 69 from 200 at epoch 19\n",
      "Loss: 2.0866756439208984\n",
      "step 79 from 200 at epoch 19\n",
      "Loss: 1.5282610654830933\n",
      "step 89 from 200 at epoch 19\n",
      "Loss: 1.5939663648605347\n",
      "step 99 from 200 at epoch 19\n",
      "Loss: 2.030796766281128\n",
      "step 109 from 200 at epoch 19\n",
      "Loss: 1.6748729944229126\n",
      "step 119 from 200 at epoch 19\n",
      "Loss: 1.4749410152435303\n",
      "step 129 from 200 at epoch 19\n",
      "Loss: 1.562962293624878\n",
      "step 139 from 200 at epoch 19\n",
      "Loss: 1.888696551322937\n",
      "step 149 from 200 at epoch 19\n",
      "Loss: 1.8741967678070068\n",
      "step 159 from 200 at epoch 19\n",
      "Loss: 1.8853713274002075\n",
      "step 169 from 200 at epoch 19\n",
      "Loss: 1.6520593166351318\n",
      "step 179 from 200 at epoch 19\n",
      "Loss: 1.8063733577728271\n",
      "step 189 from 200 at epoch 19\n",
      "Loss: 1.5491808652877808\n",
      "step 199 from 200 at epoch 19\n",
      "Loss: 2.12992262840271\n",
      "epoch 19 evaluation\n",
      "epoch loss 1.7482204306125642\n",
      "========================================================================================================\n",
      "CPU times: user 2h 1min 30s, sys: 14.6 s, total: 2h 1min 45s\n",
      "Wall time: 1h 53min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss = []\n",
    "loss_eval = []\n",
    "\n",
    "print(timestamp)\n",
    "print(f'Start training model {str(model)}')\n",
    "for i in range(epochs):\n",
    "    loss.append(train(model, training_generator, optimizer, criterion, epoch=i, writer=writer, device=device))\n",
    "    loss_eval.append(evaluate(model, valid_generator, criterion, epoch=i, writer=writer, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4751ba92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocp_models",
   "language": "python",
   "name": "ocp_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
