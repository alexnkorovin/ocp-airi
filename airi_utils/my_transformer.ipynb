{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec82612-795e-4dce-9284-f28fc73269ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f2165b-8141-4724-b2b1-0176926d4e8d",
   "metadata": {},
   "source": [
    "### Google colab\n",
    "\n",
    "This notebook can be used in colab (**this is the fastest way to run calculation on unconfigured system**):\n",
    "\n",
    "In google colab https://colab.research.google.com/ go to File | Open notebook | GitHub - \n",
    "insert the path to the current notebook and open it: https://github.com/alexnkorovin/ocp-airi/blob/dev/airi_utils/our_base_model.ipynb\n",
    "\n",
    "Before start:\n",
    "\n",
    "1. Put this shared folder with datasets in your Google Drive root folder /drive/MyDrive/\n",
    "\n",
    "This folders  can are available by the **sharing** link below:\n",
    "\n",
    "*   ocp_datasets [[ share link to drive](https://drive.google.com/drive/folders/1Nn9t-zTJiRP1-34rdAugv6aY_2-BSQfN?usp=sharing)]<br>\n",
    "\n",
    "```\n",
    "Note:\n",
    "if this folder is saved by sharing link it should contain the following files\n",
    "\n",
    "ocp-datasets/data/is2re/train/all/val_ood_both/data.lmdb\n",
    "ocp-datasets/data/is2re/train/all/test_ood_both/data.lmdb\n",
    "ocp-datasets/data/is2re/train/all/test_ood_both/structures.pkl\n",
    "\n",
    " ```\n",
    "2. Enable GPU support in Edit/Notebook Settings\n",
    "\n",
    "### on local pc\n",
    "\n",
    "download specified data files by [link](https://drive.google.com/drive/folders/1Nn9t-zTJiRP1-34rdAugv6aY_2-BSQfN?usp=sharing) into local folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d872f626-55f7-472d-9b53-c71e2f08d3df",
   "metadata": {},
   "source": [
    "### Use the cell below it to mount your google drive to dataset\n",
    " - go by the link\n",
    " - log in under your google accout\n",
    " - copy token key\n",
    " - imput it to this the imput line in this notebook"
   ]
  },
  {
   "cell_type": "raw",
   "id": "496223c0-28a4-408b-a0f7-de7e194696d3",
   "metadata": {},
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d80224-8dd8-4c54-a043-462929999211",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Enviroment installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9044a6",
   "metadata": {},
   "source": [
    "### on local pc\n",
    "```\n",
    "$ conda install pytorch-geometric -c rusty1s -c conda-forge\n",
    "```\n",
    "or via pip Wheels\n",
    "\n",
    "```\n",
    "$ python -c \"import torch; print(torch.__version__)\"\n",
    ">>> 1.9.0 - > {TORCH}=1.9.0\n",
    "python -c \"import torch; print(torch.version.cuda)\"\n",
    ">>> 11.1 - > {CUDA}=cu111\n",
    "```\n",
    "\n",
    "substite {TORCH} and {CUDA} in commands below by appropriate for your system\n",
    "```\n",
    "pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "pip install torch-geometric\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c93ce2d-3a74-4185-b186-7fdb406c1271",
   "metadata": {},
   "source": [
    "#### on colab and also local pc (but on locat preferable is conda way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d60565-7dae-4aaf-bcd1-16b0ad4bbbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This might take about 10 min in Colab (нужно только в колабе)\n",
    "# !pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.4.0+cu101.html\n",
    "# !pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.4.0+cu101.html\n",
    "# !pip install -q torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b07b096-eb2d-45e1-a7ce-0566776589b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da8e472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "from torch_geometric.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19cabffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import lmdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "643edca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_preprocessing(system, features_fields):\n",
    "    \n",
    "    tags = system['tags'].long().to(device)[:MAX_LEN]\n",
    "    tags = F.one_hot(tags, num_classes=3)\n",
    "    \n",
    "    atom_numbers = system['atomic_numbers'].long().to(device)[:MAX_LEN]\n",
    "    atom_numbers = F.one_hot(atom_numbers, num_classes=100)\n",
    "    \n",
    "    pos = system['pos'].to(device)[:MAX_LEN]\n",
    "    \n",
    "    atom_features = (tags, atom_numbers, pos)#, spherical_radii)\n",
    "    atom_embeds = torch.cat(atom_features, 1)\n",
    "                    \n",
    "    #padding\n",
    "    pad_value = -10000#0#-float(\"Inf\")\n",
    "    pads = torch.full((MAX_LEN-atom_embeds.shape[0], atom_embeds.shape[1]), pad_value)\n",
    "    padding_mask = torch.cat((torch.full((atom_embeds.shape[0], ), False), torch.full((MAX_LEN-atom_embeds.shape[0], ), True)))\n",
    "    atom_embeds = torch.cat((atom_embeds, pads))\n",
    "    \n",
    "    return (atom_embeds, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e84dea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#датасет, который умеет возвращать эелемент и собственную длину\n",
    "class Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, features_fields, target_field, type_='train', preprocessing=simple_preprocessing):\n",
    "        \n",
    "        self.data = lmdb_dataset({\"src\": data})\n",
    "        self.length = len(self.data)\n",
    "        #self.target = data[target_field]\n",
    "        self.type_ = type_\n",
    "        self.preprocessing = preprocessing\n",
    "        self.features_fields = features_fields\n",
    "        self.target = target_field\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        system = self.preprocessing(self.data[index], self.features_fields)\n",
    "        \n",
    "        if self.type_ == 'train':\n",
    "            y = self.data[index][self.target]\n",
    "            \n",
    "            return system, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e1232c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#собственно нейросеть\n",
    "class NN(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_atom=106):\n",
    "        \n",
    "        super().__init__() \n",
    "        \n",
    "        dim_hidden = 32\n",
    "        self.lin1 = nn.Linear(dim_atom, dim_hidden)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=dim_hidden, nhead=1)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=4)\n",
    "        self.lin2 = nn.Linear(dim_hidden, dim_hidden//4, bias=True)\n",
    "        self.lin3 = nn.Linear(dim_hidden//4, 1, bias=True)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        padded, src_key_padding_mask = batch[0], batch[1]\n",
    "        \n",
    "        padded = self.lin1(padded)\n",
    "                                \n",
    "        padded = padded.permute((1, 0, 2))\n",
    "        embeds = self.transformer_encoder(padded, src_key_padding_mask=src_key_padding_mask)                \n",
    "        embeds = embeds.permute((1, 0, 2))\n",
    "        \n",
    "        embeds_4 = self.lin2(embeds)\n",
    "        \n",
    "        summed = torch.sum(embeds_4, 1)\n",
    "                \n",
    "        energy = self.lin3(summed)\n",
    "        \n",
    "        return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20798f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_scalars(lr, loss, writer, step=-1, epoch=-1, type_='train'):\n",
    "    if type_ == 'train':\n",
    "        writer.add_scalar('lr per step on train', lr, step) \n",
    "        writer.add_scalar('loss per step on train', loss, step)\n",
    "    if type_ == 'val':\n",
    "        writer.add_scalar('loss per epoch on val', loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "862b1102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_hist(model, writer, step):\n",
    "    for name, weight in model.named_parameters():\n",
    "        try:\n",
    "            writer.add_histogram(name, weight, step)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc9d7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train -- ходим по батчам из итератора, обнуляем градиенты, предсказываем у, считаем лосс, считаем градиенты, делаем шаг оптимайзера, записываем лосс\n",
    "def train(model, iterator, optimizer, criterion, print_every=10, epoch=0, writer=None):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i, (systems, ys) in enumerate(iterator):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(systems).squeeze()\n",
    "        \n",
    "        loss = criterion(predictions.float(), ys.to(device).float())\n",
    "        loss.backward()     \n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = loss.item() \n",
    "        epoch_loss += batch_loss  \n",
    "        \n",
    "        if writer != None:\n",
    "            \n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            step = i + epoch*len(iterator)\n",
    "            \n",
    "            send_hist(model, writer, i)\n",
    "            send_scalars(lr, batch_loss, writer, step=step, epoch=epoch, type_='train')\n",
    "        \n",
    "        if not (i+1) % print_every:\n",
    "            print(f'step {i} from {len(iterator)} at epoch {epoch}')\n",
    "            print(f'Loss: {batch_loss}')\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a8e76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, epoch=0, writer=False):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "#    model.train(False)\n",
    "    model.eval()  \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for systems, ys in iterator:   \n",
    "\n",
    "            predictions = model(systems).squeeze()\n",
    "            loss = criterion(predictions.float(), ys.to(device).float())        \n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    overall_loss = epoch_loss / len(iterator)\n",
    "\n",
    "    if writer != None:\n",
    "        send_scalars(None, overall_loss, writer, step=None, epoch=epoch, type_='val')\n",
    "                \n",
    "    print(f'epoch loss {overall_loss}')\n",
    "            \n",
    "    return overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31952209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferens(model, iterator):\n",
    "    y = torch.tensor([])\n",
    "\n",
    "#    model.train(False)\n",
    "    model.eval()  \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for systems in iterator:   \n",
    "          predictions = model(systems).squeeze()\n",
    "          y = torch.cat((y, predictions))\n",
    "      \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c98223",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a6fc5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for colab\n",
    "# train_dataset_file_path = \"/content/drive/MyDrive/ocp_datasets/data/is2re/10k/train/structures_train.pkl\"\n",
    "\n",
    "# user specific folder\n",
    "train_dataset_file_path = os.path.expanduser(\"../../ocp_datasets_ssd/data/is2re/100k/train/data_mod.lmdb\")\n",
    "# train_dataset_file_path= \"../../ocp_datasets/data/is2re/10k/train/structures_tain.pkl\"\n",
    "\n",
    "val_dataset_file_path = os.path.expanduser(\"../../ocp_datasets_ssd/data/is2re/all/val_ood_both/data_mod.lmdb\")\n",
    "#val_dataset_file_path = os.path.expanduser(\"~/Downloads/structures_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "476cdcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_workers = 0\n",
    "MAX_LEN = 300\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c15ae949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_cols = ['atomic_numbers', 'edge_index_new', 'distances_new', \n",
    "#                  'contact_solid_angles', 'tags', 'voronoi_volumes', 'spherical_domain_radii']\n",
    "\n",
    "features_cols = ['pos', 'atomic_numbers', 'tags']\n",
    "\n",
    "target_col = 'y_relaxed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb945e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#инициализируем тренировочный датасети и тренировочный итератор\n",
    "training_set = Dataset(train_dataset_file_path, features_cols, target_col)\n",
    "training_generator = DataLoader(training_set, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11d078bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#инициализируем валидационный датасет и валидационный итератор\n",
    "valid_set = Dataset(val_dataset_file_path, features_cols, target_col)\n",
    "valid_generator = DataLoader(valid_set, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ca059",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0046a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#чтобы тензор по умолчанию заводился на куде\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    print('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98fa2809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f93e4f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "model = NN(dim_atom=next(iter(training_generator))[0][0].shape[2])\n",
    "\n",
    "#optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "#переносим на куду если она есть\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc1c0ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-08-09-54-11\n"
     ]
    }
   ],
   "source": [
    "timestamp = str(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a1cbede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard writer, при первом запуске надо руками сделать папку для логов\n",
    "\n",
    "# server\n",
    "#log_folder_path = \"../../ocp_results/logs/tensorboard/out_base_model\"\n",
    "\n",
    "# colab\n",
    "# log_folder_path = \"/content/drive/MyDrive/ocp_results/logs/tensorboard/out_base_model\"\n",
    "\n",
    "# user_specific \n",
    "log_file_path = \"../logs/tensorboard_airi\"\n",
    "\n",
    "writer = SummaryWriter(log_file_path + '/' + timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e477b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logfile_str = {\n",
    "    \"train_dataset_file_path\": train_dataset_file_path,\n",
    "    \"val_dataset_file_path\": val_dataset_file_path,\n",
    "    \"features_cols\": features_cols,\n",
    "    \"target_col\": target_col,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"epochs\": epochs,\n",
    "}\n",
    "\n",
    "#граф модели\n",
    "trace_system = next(iter(training_generator))[0]\n",
    "writer.add_graph(model, (trace_system,))\n",
    "writer.add_text(timestamp, str(logfile_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aca95f2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6c6fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-08-09-54-11\n",
      "epoch 0\n",
      "step 9 from 782 at epoch 0\n",
      "Loss: 39.70013427734375\n",
      "step 19 from 782 at epoch 0\n",
      "Loss: 13.021162033081055\n",
      "step 29 from 782 at epoch 0\n",
      "Loss: 5.784801483154297\n",
      "step 39 from 782 at epoch 0\n",
      "Loss: 4.738654613494873\n",
      "step 49 from 782 at epoch 0\n",
      "Loss: 2.828641891479492\n",
      "step 59 from 782 at epoch 0\n",
      "Loss: 1.8550431728363037\n",
      "step 69 from 782 at epoch 0\n",
      "Loss: 2.000392436981201\n",
      "step 79 from 782 at epoch 0\n",
      "Loss: 1.6742053031921387\n",
      "step 89 from 782 at epoch 0\n",
      "Loss: 1.8906172513961792\n",
      "step 99 from 782 at epoch 0\n",
      "Loss: 1.8634231090545654\n",
      "step 109 from 782 at epoch 0\n",
      "Loss: 2.018066883087158\n",
      "step 119 from 782 at epoch 0\n",
      "Loss: 1.968219518661499\n",
      "step 129 from 782 at epoch 0\n",
      "Loss: 1.7375426292419434\n",
      "step 139 from 782 at epoch 0\n",
      "Loss: 2.1935906410217285\n",
      "step 149 from 782 at epoch 0\n",
      "Loss: 2.2990899085998535\n",
      "step 159 from 782 at epoch 0\n",
      "Loss: 2.175541639328003\n",
      "step 169 from 782 at epoch 0\n",
      "Loss: 2.388439178466797\n",
      "step 179 from 782 at epoch 0\n",
      "Loss: 1.7567169666290283\n",
      "step 189 from 782 at epoch 0\n",
      "Loss: 1.993103265762329\n",
      "step 199 from 782 at epoch 0\n",
      "Loss: 2.109551191329956\n",
      "step 209 from 782 at epoch 0\n",
      "Loss: 1.9241361618041992\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss = []\n",
    "loss_eval = []\n",
    "\n",
    "print(timestamp)\n",
    "#print(f'Start training model {str(model)}')\n",
    "for i in range(epochs):\n",
    "    print(f'epoch {i}')\n",
    "    loss.append(train(model, training_generator, optimizer, criterion, epoch=i, writer=writer))\n",
    "    print(f'validation on epoch {i} starts')\n",
    "    loss_eval.append(evaluate(model, valid_generator, criterion, epoch=i, writer=writer))\n",
    "    print('=========================================================================================================')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d5cee00-5566-4254-8d22-25e65345b8ec",
   "metadata": {},
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c191ef8-d08c-4c44-89cd-7d358d676b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_reval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocp_models",
   "language": "python",
   "name": "ocp_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
