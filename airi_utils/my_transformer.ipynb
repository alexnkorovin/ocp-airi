{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e951e5-1f6f-4c50-9e49-12acc51134f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Colab only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec82612-795e-4dce-9284-f28fc73269ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f2165b-8141-4724-b2b1-0176926d4e8d",
   "metadata": {},
   "source": [
    "### Google colab\n",
    "\n",
    "This notebook can be used in colab (**this is the fastest way to run calculation on unconfigured system**):\n",
    "\n",
    "In google colab https://colab.research.google.com/ go to File | Open notebook | GitHub - \n",
    "insert the path to the current notebook and open it: https://github.com/alexnkorovin/ocp-airi/blob/dev/airi_utils/our_base_model.ipynb\n",
    "\n",
    "Before start:\n",
    "\n",
    "1. Put this shared folder with datasets in your Google Drive root folder /drive/MyDrive/\n",
    "\n",
    "This folders  can are available by the **sharing** link below:\n",
    "\n",
    "*   ocp_datasets [[ share link to drive](https://drive.google.com/drive/folders/1Nn9t-zTJiRP1-34rdAugv6aY_2-BSQfN?usp=sharing)]<br>\n",
    "\n",
    "```\n",
    "Note:\n",
    "if this folder is saved by sharing link it should contain the following files\n",
    "\n",
    "ocp-datasets/data/is2re/train/all/val_ood_both/data.lmdb\n",
    "ocp-datasets/data/is2re/train/all/test_ood_both/data.lmdb\n",
    "ocp-datasets/data/is2re/train/all/test_ood_both/structures.pkl\n",
    "\n",
    " ```\n",
    "2. Enable GPU support in Edit/Notebook Settings\n",
    "\n",
    "### on local pc\n",
    "\n",
    "download specified data files by [link](https://drive.google.com/drive/folders/1Nn9t-zTJiRP1-34rdAugv6aY_2-BSQfN?usp=sharing) into local folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d872f626-55f7-472d-9b53-c71e2f08d3df",
   "metadata": {},
   "source": [
    "### Use the cell below it to mount your google drive to dataset\n",
    " - go by the link\n",
    " - log in under your google accout\n",
    " - copy token key\n",
    " - imput it to this the imput line in this notebook"
   ]
  },
  {
   "cell_type": "raw",
   "id": "496223c0-28a4-408b-a0f7-de7e194696d3",
   "metadata": {},
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d80224-8dd8-4c54-a043-462929999211",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Enviroment installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9044a6",
   "metadata": {},
   "source": [
    "### on local pc\n",
    "```\n",
    "$ conda install pytorch-geometric -c rusty1s -c conda-forge\n",
    "```\n",
    "or via pip Wheels\n",
    "\n",
    "```\n",
    "$ python -c \"import torch; print(torch.__version__)\"\n",
    ">>> 1.9.0 - > {TORCH}=1.9.0\n",
    "python -c \"import torch; print(torch.version.cuda)\"\n",
    ">>> 11.1 - > {CUDA}=cu111\n",
    "```\n",
    "\n",
    "substite {TORCH} and {CUDA} in commands below by appropriate for your system\n",
    "```\n",
    "pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "pip install torch-geometric\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c93ce2d-3a74-4185-b186-7fdb406c1271",
   "metadata": {},
   "source": [
    "#### on colab and also local pc (but on locat preferable is conda way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d60565-7dae-4aaf-bcd1-16b0ad4bbbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This might take about 10 min in Colab (нужно только в колабе)\n",
    "# !pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.4.0+cu101.html\n",
    "# !pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.4.0+cu101.html\n",
    "# !pip install -q torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b07b096-eb2d-45e1-a7ce-0566776589b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d1f0b-77db-4bba-a495-9c73e60e9d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "from torch_geometric.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dataloader import lmdb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cf63ae-8991-47a2-a0a1-c6292dcbaf40",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9acafa-0e6e-46c2-997a-3639a6a222c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTASETS\n",
    "train_dataset_file_path = os.path.expanduser(\"../../ocp_datasets_ssd/data/is2re/100k/train/data_mod.lmdb\")\n",
    "val_dataset_file_path = os.path.expanduser(\"../../ocp_datasets_ssd/data/is2re/all/val_ood_both/data_mod.lmdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b908ff-a806-4352-a184-d989a576aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "batch_size = 81\n",
    "num_workers = 0\n",
    "MAX_LEN = 300\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2027e55c-61a6-46c1-b354-bd4027978696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEAUTURES\n",
    "features_cols = ['pos', 'atomic_numbers', 'tags', 'voronoi_volumes', 'voronoi_surface_areas', 'spherical_domain_radii']\n",
    "target_col = 'y_relaxed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c365ca-4d63-4a2b-b065-a835602e0815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOG_Path\n",
    "log_file_path = \"../logs/tensorboard_airi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e51641-00b3-4cc4-a235-4f956062fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS for logging\n",
    "logfile_str = {\n",
    "    \"train_dataset_file_path\": train_dataset_file_path,\n",
    "    \"val_dataset_file_path\": val_dataset_file_path,\n",
    "    \"features_cols\": features_cols,\n",
    "    \"target_col\": target_col,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"epochs\": epochs,\n",
    "    \"MAX_LEN\": MAX_LEN\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912738a2-a726-46cb-bdf4-9c6bc5bb38d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# All Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d0644-addb-42d8-8a55-1edb7911c34d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376fb148-f10a-4e68-8631-d6b9f4a7ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_reshape(tensor):\n",
    "    return torch.reshape(tensor, (tensor.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643edca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_preprocessing(system, features_fields):\n",
    "    \n",
    "    tags = system['tags'].long().to(device)[:MAX_LEN]\n",
    "    tags = F.one_hot(tags, num_classes=3)\n",
    "    \n",
    "    atom_numbers = system['atomic_numbers'].long().to(device)[:MAX_LEN]\n",
    "    atom_numbers = F.one_hot(atom_numbers, num_classes=100)\n",
    "    \n",
    "    pos = system['pos'].to(device)[:MAX_LEN]\n",
    "    \n",
    "    v_volumes = system['voronoi_volumes'].float().to(device)\n",
    "    v_volumes = my_reshape(v_volumes)\n",
    "    \n",
    "    v_areas = system['voronoi_surface_areas'].float().to(device)\n",
    "    v_areas = my_reshape(v_areas)\n",
    "    \n",
    "    spherical_radii = system['spherical_domain_radii'].float().to(device)\n",
    "    spherical_radii = my_reshape(spherical_radii)\n",
    "    \n",
    "    atom_features = (tags, atom_numbers, pos, v_volumes, v_areas, spherical_radii)\n",
    "    atom_embeds = torch.cat(atom_features, 1)\n",
    "                    \n",
    "    #padding\n",
    "    pad_value = -10000#0#-float(\"Inf\")\n",
    "    pads = torch.full((MAX_LEN-atom_embeds.shape[0], atom_embeds.shape[1]), pad_value)\n",
    "    padding_mask = torch.cat((torch.full((atom_embeds.shape[0], ), False), torch.full((MAX_LEN-atom_embeds.shape[0], ), True)))\n",
    "    atom_embeds = torch.cat((atom_embeds, pads))\n",
    "    \n",
    "    return (atom_embeds, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84dea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#датасет, который умеет возвращать эелемент и собственную длину\n",
    "class Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, features_fields, target_field, type_='train', preprocessing=simple_preprocessing):\n",
    "        \n",
    "        self.data = lmdb_dataset({\"src\": data})\n",
    "        self.length = len(self.data)\n",
    "        #self.target = data[target_field]\n",
    "        self.type_ = type_\n",
    "        self.preprocessing = preprocessing\n",
    "        self.features_fields = features_fields\n",
    "        self.target = target_field\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        system = self.preprocessing(self.data[index], self.features_fields)\n",
    "        \n",
    "        if self.type_ == 'train':\n",
    "            y = self.data[index][self.target]\n",
    "            \n",
    "            return system, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1232c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#собственно нейросеть\n",
    "class NN(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_atom=106):\n",
    "        \n",
    "        super().__init__() \n",
    "        \n",
    "        dim_hidden = 32\n",
    "        self.lin1 = nn.Linear(dim_atom, dim_hidden)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=dim_hidden, nhead=1)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=4)\n",
    "        self.lin2 = nn.Linear(dim_hidden, dim_hidden//4, bias=True)\n",
    "        self.lin3 = nn.Linear(dim_hidden//4, 1, bias=True)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        padded, src_key_padding_mask = batch[0], batch[1]\n",
    "        \n",
    "        padded = self.lin1(padded)\n",
    "                                \n",
    "        padded = padded.permute((1, 0, 2))\n",
    "        embeds = self.transformer_encoder(padded, src_key_padding_mask=src_key_padding_mask)                \n",
    "        embeds = embeds.permute((1, 0, 2))\n",
    "        \n",
    "        embeds_4 = self.lin2(embeds)\n",
    "        \n",
    "        summed = torch.sum(embeds_4, 1)\n",
    "                \n",
    "        energy = self.lin3(summed)\n",
    "        \n",
    "        return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20798f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_scalars(lr, loss, writer, step=-1, epoch=-1, type_='train'):\n",
    "    if type_ == 'train':\n",
    "        writer.add_scalar('lr per step on train', lr, step) \n",
    "        writer.add_scalar('loss per step on train', loss, step)\n",
    "    if type_ == 'val':\n",
    "        writer.add_scalar('loss per epoch on val', loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b1102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_hist(model, writer, step):\n",
    "    for name, weight in model.named_parameters():\n",
    "        try:\n",
    "            writer.add_histogram(name, weight, step)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train -- ходим по батчам из итератора, обнуляем градиенты, предсказываем у, считаем лосс, считаем градиенты, делаем шаг оптимайзера, записываем лосс\n",
    "def train(model, iterator, optimizer, criterion, print_every=10, epoch=0, writer=None):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i, (systems, ys) in enumerate(iterator):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(systems).squeeze()\n",
    "        \n",
    "        loss = criterion(predictions.float(), ys.to(device).float())\n",
    "        loss.backward()     \n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = loss.item() \n",
    "        epoch_loss += batch_loss  \n",
    "        \n",
    "        if writer != None:\n",
    "            \n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            step = i + epoch*len(iterator)\n",
    "            \n",
    "            send_hist(model, writer, i)\n",
    "            send_scalars(lr, batch_loss, writer, step=step, epoch=epoch, type_='train')\n",
    "        \n",
    "        if not (i+1) % print_every:\n",
    "            print(f'step {i} from {len(iterator)} at epoch {epoch}')\n",
    "            print(f'Loss: {batch_loss}')\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, epoch=0, writer=False):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "#    model.train(False)\n",
    "    model.eval()  \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for systems, ys in iterator:   \n",
    "\n",
    "            predictions = model(systems).squeeze()\n",
    "            loss = criterion(predictions.float(), ys.to(device).float())        \n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    overall_loss = epoch_loss / len(iterator)\n",
    "\n",
    "    if writer != None:\n",
    "        send_scalars(None, overall_loss, writer, step=None, epoch=epoch, type_='val')\n",
    "                \n",
    "    print(f'epoch loss {overall_loss}')\n",
    "            \n",
    "    return overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31952209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferens(model, iterator):\n",
    "    y = torch.tensor([])\n",
    "\n",
    "#    model.train(False)\n",
    "    model.eval()  \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for systems in iterator:   \n",
    "          predictions = model(systems).squeeze()\n",
    "          y = torch.cat((y, predictions))\n",
    "      \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c98223",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb945e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#инициализируем тренировочный датасети и тренировочный итератор\n",
    "training_set = Dataset(train_dataset_file_path, features_cols, target_col)\n",
    "training_generator = DataLoader(training_set, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab449b6-691b-496e-ba34-d3d9e72d2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d078bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#инициализируем валидационный датасет и валидационный итератор\n",
    "valid_set = Dataset(val_dataset_file_path, features_cols, target_col)\n",
    "valid_generator = DataLoader(valid_set, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ca059",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## MODEL CORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0046a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#чтобы тензор по умолчанию заводился на куде\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    print('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa2809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e4f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "model = NN(dim_atom=next(iter(training_generator))[0][0].shape[2])\n",
    "\n",
    "#optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "#переносим на куду если она есть\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1c0ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = str(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1cbede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard writer, при первом запуске надо руками сделать папку для логов\n",
    "\n",
    "# server\n",
    "# log_folder_path = \"../../ocp_results/logs/tensorboard/out_base_model\"\n",
    "\n",
    "# colab\n",
    "# log_folder_path = \"/content/drive/MyDrive/ocp_results/logs/tensorboard/out_base_model\"\n",
    "\n",
    "# user_specific \n",
    "writer = SummaryWriter(log_file_path + '/' + timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e477b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# граф модели\n",
    "trace_system = next(iter(training_generator))[0]\n",
    "writer.add_graph(model, (trace_system,))\n",
    "writer.add_text(timestamp, str(logfile_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aca95f2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6c6fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "loss = []\n",
    "loss_eval = []\n",
    "\n",
    "print(timestamp)\n",
    "#print(f'Start training model {str(model)}')\n",
    "for i in range(epochs):\n",
    "    print(f'epoch {i}')\n",
    "    loss.append(train(model, training_generator, optimizer, criterion, epoch=i, writer=writer))\n",
    "    print(f'validation on epoch {i} starts')\n",
    "    loss_eval.append(evaluate(model, valid_generator, criterion, epoch=i, writer=writer))\n",
    "    print('=========================================================================================================')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d5cee00-5566-4254-8d22-25e65345b8ec",
   "metadata": {},
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c191ef8-d08c-4c44-89cd-7d358d676b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bec138-b41e-44ec-bd5e-93098034bd41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocp_models",
   "language": "python",
   "name": "ocp_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
