{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim\r\n",
    "\r\n",
    "\r\n",
    "from datetime import datetime\r\n",
    "from torch import nn\r\n",
    "from torch_geometric.data import Data, DataLoader\r\n",
    "from torch_geometric.nn import MessagePassing\r\n",
    "from torch_scatter import scatter\r\n",
    "from torch.utils.tensorboard import SummaryWriter\r\n",
    "\r\n",
    "from DataClasses import lmdb_dataset, Dataset\r\n",
    "from ModelFunctions import train, evaluate, inference"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#вызывается каждый раз, когда датасет отдаёт элемент (систему)\r\n",
    "#делаем из данных матрицу векторов-атомов, список рёбер (edge_index) и матрицу векторов-рёбер; надо писать свою функцию для каждой сети\r\n",
    "def preprocessing(system):\r\n",
    "    \r\n",
    "    atom_embeds = torch.cat(atom_features, 1)\r\n",
    "    \r\n",
    "    edge_index = system['edge_index_new'].long()\r\n",
    "\r\n",
    "    edges_embeds = torch.cat(edge_features, 1)\r\n",
    "    \r\n",
    "    return Data(x=atom_embeds.to(device), edge_index=edge_index.to(device), edge_attr=edges_embeds.to(device))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\r\n",
    "\\mathbf{x}_i^{(k)} = \\gamma^{(k)} \\left( \\mathbf{x}_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{j,i}\\right) \\right)\r\n",
    "$$\r\n",
    "\r\n",
    "Гамма лежит в апдейт, квадратик в aggr, а фи в месседж; в этом примере квадратик -- суммирование"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class DistanceBlock(torch.nn.Module):\r\n",
    "    def __init__(self, in_channels, out_channels, max_num_elements, scalar_max):\r\n",
    "        super.__init__()\r\n",
    "        self.in_channels = in_channels\r\n",
    "        self.out_channels = out_channels\r\n",
    "        self.max_num_elements = max_num_elements\r\n",
    "\r\n",
    "        self.fc1 = nn.Linear(self.in_channels, self.out_channels)\r\n",
    "\r\n",
    "    def forward(self, lmdb_element):\r\n",
    "        x = lmdb_dataset[\"distances_new\"]\r\n",
    "        x = self.fc1(x)\r\n",
    "        return x\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class ProjectLatLongSphere(torch.nn.Module):\r\n",
    "    def __init__(self, sphere_size_lat, sphere_size_long):\r\n",
    "        super(ProjectLatLongSphere, self).__init__()\r\n",
    "        self.sphere_size_lat = sphere_size_lat\r\n",
    "        self.sphere_size_long = sphere_size_long\r\n",
    "\r\n",
    "    def forward(self, x, length, index, delta, source_edge_index):\r\n",
    "        device = x.device\r\n",
    "        hidden_channels = len(x[0])\r\n",
    "\r\n",
    "        x_proj = torch.zeros(\r\n",
    "            length * self.sphere_size_lat * self.sphere_size_long,\r\n",
    "            hidden_channels,\r\n",
    "            device=device,\r\n",
    "        )\r\n",
    "        splat_values = x[source_edge_index]\r\n",
    "\r\n",
    "        # Perform bilinear splatting\r\n",
    "        x_proj.index_add_(0, index[0], splat_values * (delta[0].view(-1, 1)))\r\n",
    "        x_proj.index_add_(0, index[1], splat_values * (delta[1].view(-1, 1)))\r\n",
    "        x_proj.index_add_(0, index[2], splat_values * (delta[2].view(-1, 1)))\r\n",
    "        x_proj.index_add_(0, index[3], splat_values * (delta[3].view(-1, 1)))\r\n",
    "\r\n",
    "        x_proj = x_proj.view(\r\n",
    "            length,\r\n",
    "            self.sphere_size_lat * self.sphere_size_long,\r\n",
    "            hidden_channels,\r\n",
    "        )\r\n",
    "        x_proj = torch.transpose(x_proj, 1, 2).contiguous()\r\n",
    "        x_proj = x_proj.view(\r\n",
    "            length,\r\n",
    "            hidden_channels,\r\n",
    "            self.sphere_size_lat,\r\n",
    "            self.sphere_size_long,\r\n",
    "        )\r\n",
    "\r\n",
    "        return x_proj"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class SpinConvBlock(torch.nn.Module):\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        in_hidden_channels,\r\n",
    "        mid_hidden_channels,\r\n",
    "        sphere_size_lat,\r\n",
    "        sphere_size_long,\r\n",
    "        act,\r\n",
    "        lmax\r\n",
    "    ):\r\n",
    "        super(self).__init__()\r\n",
    "        self.in_hidden_channels = in_hidden_channels\r\n",
    "        self.mid_hidden_channels = mid_hidden_channels\r\n",
    "        self.sphere_size_lat = sphere_size_lat\r\n",
    "        self.sphere_size_long = sphere_size_long\r\n",
    "        # self.sphere_message = sphere_message\r\n",
    "        self.act = act\r\n",
    "        self.lmax = lmax\r\n",
    "        self.num_groups = self.in_hidden_channels // 8\r\n",
    "\r\n",
    "        self.ProjectLatLongSphere = ProjectLatLongSphere(\r\n",
    "            sphere_size_lat, sphere_size_long\r\n",
    "        )\r\n",
    "        # assert self.sphere_message in [\r\n",
    "        #     \"fullconv\",\r\n",
    "        #     \"rotspharmwd\",\r\n",
    "        # ]\r\n",
    "\r\n",
    "        # if self.sphere_message == \"fullconv\":\r\n",
    "        padding = self.sphere_size_long // 2\r\n",
    "        self.conv1 = nn.Conv1d(\r\n",
    "            self.in_hidden_channels * self.sphere_size_lat,\r\n",
    "            self.mid_hidden_channels,\r\n",
    "            self.sphere_size_long,\r\n",
    "            groups=self.in_hidden_channels // 8,\r\n",
    "            padding=padding,\r\n",
    "            padding_mode=\"circular\",\r\n",
    "        )\r\n",
    "        self.pool = nn.AvgPool1d(sphere_size_long)\r\n",
    "\r\n",
    "        self.GroupNorm = nn.GroupNorm(\r\n",
    "            self.num_groups, self.mid_hidden_channels\r\n",
    "        )\r\n",
    "\r\n",
    "    def forward(self, x, out_size, proj_index, proj_delta, proj_src_index):\r\n",
    "        x = self.ProjectLatLongSphere(\r\n",
    "            x, out_size, proj_index, proj_delta, proj_src_index\r\n",
    "        )\r\n",
    "\r\n",
    "        if self.sphere_message in [\"fullconv\"]:\r\n",
    "            x = x.view(\r\n",
    "                -1,\r\n",
    "                self.in_hidden_channels * self.sphere_size_lat,\r\n",
    "                self.sphere_size_long,\r\n",
    "            )\r\n",
    "            x = self.conv1(x)\r\n",
    "            x = self.act(x)\r\n",
    "            # Pool in the longitudal direction\r\n",
    "            x = self.pool(x[:, :, 0 : self.sphere_size_long])\r\n",
    "            x = x.view(out_size, -1)\r\n",
    "\r\n",
    "        x = self.GroupNorm(x)\r\n",
    "\r\n",
    "        return x\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class EmbeddingBlock(torch.nn.Module): #TODO перепроверить все названия переменных\r\n",
    "    def __init__(self, embedding_dim, embedded_hidden, element_number, fc1_input,  fc1_to_fc2_hidden, fc2_embedded_hidden, fc3_out, activation):\r\n",
    "        super().__init__()\r\n",
    "        self.max_number_of_elements = element_number\r\n",
    "        \r\n",
    "        self.embeding = nn.Embedding(num_embeddings=self.max_number_of_elements, embedding_dim=embedding_dim)\r\n",
    "\r\n",
    "        nn.init.uniform_(self.source_embed.weight.data, -0.0001, 0.0001)\r\n",
    "        nn.init.uniform_(self.target_embed.weight.data, -0.0001, 0.0001)\r\n",
    "\r\n",
    "        self.embedding_dim = embedding_dim\r\n",
    "        self.embedding_hidden = embedded_hidden\r\n",
    "        self.fc_embedding = nn.Linear(2 * self.embedding_dim, embedded_hidden)\r\n",
    "        \r\n",
    "        self.fc2_embedded_hidden = fc2_embedded_hidden\r\n",
    "\r\n",
    "        self.fc1 = nn.Linear(fc1_input, fc1_to_fc2_hidden)\r\n",
    "        self.fc2 = nn.Linear(fc1_to_fc2_hidden, fc2_embedded_hidden*self.embedding_dim)\r\n",
    "\r\n",
    "        self.fc3 = nn.Linear(fc2_embedded_hidden, fc3_out)\r\n",
    "\r\n",
    "        self.softmax = nn.Softmax(dim=1)\r\n",
    "\r\n",
    "        self.activation = activation\r\n",
    "\r\n",
    "    # def buildEmbed(x, connectivity):\r\n",
    "    #     for i in \r\n",
    "\r\n",
    "    def forward(self, batch):\r\n",
    "        \r\n",
    "        x = torch.tensor(batch['x'], dtype=torch.long)\r\n",
    "        edge_index = torch.tensor(batch['edge_index'], dtype=torch.long)\r\n",
    "        edge_attr = torch.tensor(batch['edge_attr'], dtype=torch.long)\r\n",
    "\r\n",
    "        source_embedding = self.embeding(x[edge_index[0]])\r\n",
    "        target_embedding = self.embeding(x[edge_index[1]])\r\n",
    "        embedding = torch.cat([source_embedding, target_embedding], dim=1)\r\n",
    "        embedding = self.fc_embedding(embedding)\r\n",
    "        embedding = self.softmax(embedding)\r\n",
    "\r\n",
    "\r\n",
    "        x = self.fc1(x)\r\n",
    "        x = self.activation(x)\r\n",
    "        x = self.fc2(x)\r\n",
    "        x = self.activation(x)\r\n",
    "        \r\n",
    "        #Тута будет ошибка\r\n",
    "        x = (\r\n",
    "            x.view(-1, self.embedding_hidden, self.fc2_embedded_hidden)\r\n",
    "        ) * (embedding.view(-1, self.embedding_hidden, 1))\r\n",
    "\r\n",
    "        x = torch.sum(x, dim=1)\r\n",
    "        x = self.fc3(x)\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MessageBlock(torch.nn.Module):\r\n",
    "    def __init__(self, in_hidden_channels, out_hidden_channels, mid_hidden_channels,\r\n",
    "     embedding_size, sphere_size_lat, sphere_size_long, max_num_elements, act, lmax):\r\n",
    "        super().__init__()\r\n",
    "        self.in_hidden_channels = in_hidden_channels\r\n",
    "        self.out_hidden_channels = out_hidden_channels\r\n",
    "        self.mid_hidden_channels = mid_hidden_channels\r\n",
    "        self.act = act\r\n",
    "        self.lmax = lmax\r\n",
    "        self.embedding_size = embedding_size\r\n",
    "        self.sphere_size_lat = sphere_size_lat\r\n",
    "        self.sphere_size_long = sphere_size_long\r\n",
    "        self.max_num_elements = max_num_elements\r\n",
    "        self.num_embedding_basis = 8\r\n",
    "\r\n",
    "        self.spinconvblock = SpinConvBlock(\r\n",
    "            self.in_hidden_channels, self.mid_hidden_channels,\r\n",
    "            self.sphere_size_lat, sphere_size_long, self.act,\r\n",
    "            self.lmax\r\n",
    "        )\r\n",
    "\r\n",
    "        self.embeddingblock1 = EmbeddingBlock(\r\n",
    "            fc1_input = self.mid_hidden_channels,\r\n",
    "            fc1_to_fc2_hidden = self.mid_hidden_channels,\r\n",
    "            fc2_embedded_hidden = self.mid_hidden_channels,\r\n",
    "            fc3_out = self.mid_hidden_channels,\r\n",
    "            embedding_dim = self.embedding_size, \r\n",
    "            embedded_hidden = self.num_embedding_basis, \r\n",
    "            activation = self.act\r\n",
    "        )\r\n",
    "\r\n",
    "        self.embeddingblock2 = EmbeddingBlock(\r\n",
    "            fc1_input = self.mid_hidden_channels,\r\n",
    "            fc1_to_fc2_hidden = self.mid_hidden_channels,\r\n",
    "            fc2_embedded_hidden = self.mid_hidden_channels,\r\n",
    "            fc3_out = self.out_hidden_channels\r\n",
    "            embedding_dim = self.embedding_size, \r\n",
    "            embedded_hidden = self.num_embedding_basis, \r\n",
    "            activation = self.act\r\n",
    "        )\r\n",
    "\r\n",
    "        self.distfc1 = nn.Linear(self.mid_hidden_channels, self.mid_hidden_channels)\r\n",
    "        self.distfc2 = nn.Linear(self.mid_hidden_channels, self.mid_hidden_channels)\r\n",
    "\r\n",
    "    def forward(self, x, x_dist, source_element, target_element, proj_index, proj_delta, proj_src_index):\r\n",
    "                out_size = len(x)\r\n",
    "\r\n",
    "        x = self.spinconvblock(\r\n",
    "            x, out_size, proj_index, proj_delta, proj_src_index\r\n",
    "        )\r\n",
    "\r\n",
    "        x = self.embeddingblock1(x, source_element, target_element)\r\n",
    "\r\n",
    "        x_dist = self.distfc1(x_dist)\r\n",
    "        x_dist = self.act(x_dist)\r\n",
    "        x_dist = self.distfc2(x_dist)\r\n",
    "        x = x + x_dist\r\n",
    "\r\n",
    "        x = self.act(x)\r\n",
    "        x = self.embeddingblock2(x, source_element, target_element)\r\n",
    "\r\n",
    "        return x\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@registry.register_model(\"spinconv\")\r\n",
    "class spinconv(BaseModel):\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        num_atoms,  # not used\r\n",
    "        bond_feat_dim,  # not used\r\n",
    "        num_targets,\r\n",
    "        use_pbc=True,\r\n",
    "        regress_forces=True,\r\n",
    "        otf_graph=False,\r\n",
    "        hidden_channels=32,\r\n",
    "        mid_hidden_channels=200,\r\n",
    "        num_interactions=1,\r\n",
    "        num_basis_functions=200,\r\n",
    "        basis_width_scalar=1.0,\r\n",
    "        max_num_neighbors=20,\r\n",
    "        sphere_size_lat=15,\r\n",
    "        sphere_size_long=9,\r\n",
    "        cutoff=10.0,\r\n",
    "        distance_block_scalar_max=2.0,\r\n",
    "        max_num_elements=90,\r\n",
    "        embedding_size=32,\r\n",
    "        show_timing_info=False,\r\n",
    "        sphere_message=\"fullconv\",  # message block sphere representation\r\n",
    "        output_message=\"fullconv\",  # output block sphere representation\r\n",
    "        lmax=False,\r\n",
    "        force_estimator=\"random\",\r\n",
    "        model_ref_number=0,\r\n",
    "        readout=\"add\",\r\n",
    "        num_rand_rotations=5,\r\n",
    "        scale_distances=True,\r\n",
    "    ):\r\n",
    "        super(spinconv, self).__init__()\r\n",
    "\r\n",
    "        self.num_targets = num_targets\r\n",
    "        self.num_random_rotations = num_rand_rotations\r\n",
    "        self.regress_forces = regress_forces\r\n",
    "        self.use_pbc = use_pbc\r\n",
    "        self.cutoff = cutoff\r\n",
    "        self.otf_graph = otf_graph\r\n",
    "        self.show_timing_info = show_timing_info\r\n",
    "        self.max_num_elements = max_num_elements\r\n",
    "        self.mid_hidden_channels = mid_hidden_channels\r\n",
    "        self.sphere_size_lat = sphere_size_lat\r\n",
    "        self.sphere_size_long = sphere_size_long\r\n",
    "        self.num_atoms = 0\r\n",
    "        self.hidden_channels = hidden_channels\r\n",
    "        self.embedding_size = embedding_size\r\n",
    "        self.max_num_neighbors = max_num_neighbors\r\n",
    "        self.sphere_message = sphere_message\r\n",
    "        self.output_message = output_message\r\n",
    "        self.force_estimator = force_estimator\r\n",
    "        self.num_basis_functions = num_basis_functions\r\n",
    "        self.distance_block_scalar_max = distance_block_scalar_max\r\n",
    "        self.grad_forces = False\r\n",
    "        self.num_embedding_basis = 8\r\n",
    "        self.lmax = lmax\r\n",
    "        self.scale_distances = scale_distances\r\n",
    "        self.basis_width_scalar = basis_width_scalar\r\n",
    "\r\n",
    "        if self.sphere_message in [\"spharm\", \"rotspharmroll\", \"rotspharmwd\"]:\r\n",
    "            assert self.lmax, \"lmax must be defined for spherical harmonics\"\r\n",
    "        if self.output_message in [\"spharm\", \"rotspharmroll\", \"rotspharmwd\"]:\r\n",
    "            assert self.lmax, \"lmax must be defined for spherical harmonics\"\r\n",
    "\r\n",
    "        # variables used for display purposes\r\n",
    "        self.counter = 0\r\n",
    "        self.start_time = time.time()\r\n",
    "        self.total_time = 0\r\n",
    "        self.model_ref_number = model_ref_number\r\n",
    "\r\n",
    "        if self.force_estimator == \"grad\":\r\n",
    "            self.grad_forces = True\r\n",
    "\r\n",
    "        # self.act = ShiftedSoftplus()\r\n",
    "        self.act = Swish()\r\n",
    "\r\n",
    "        self.distance_expansion_forces = GaussianSmearing(\r\n",
    "            0.0,\r\n",
    "            cutoff,\r\n",
    "            num_basis_functions,\r\n",
    "            basis_width_scalar,\r\n",
    "        )\r\n",
    "\r\n",
    "        # Weights for message initialization\r\n",
    "        self.embeddingblock2 = EmbeddingBlock(\r\n",
    "            self.mid_hidden_channels,\r\n",
    "            self.hidden_channels,\r\n",
    "            self.mid_hidden_channels,\r\n",
    "            self.embedding_size,\r\n",
    "            self.num_embedding_basis,\r\n",
    "            self.max_num_elements,\r\n",
    "            self.act,\r\n",
    "        )\r\n",
    "        self.distfc1 = nn.Linear(\r\n",
    "            self.mid_hidden_channels, self.mid_hidden_channels\r\n",
    "        )\r\n",
    "        self.distfc2 = nn.Linear(\r\n",
    "            self.mid_hidden_channels, self.mid_hidden_channels\r\n",
    "        )\r\n",
    "\r\n",
    "        self.dist_block = DistanceBlock(\r\n",
    "            self.num_basis_functions,\r\n",
    "            self.mid_hidden_channels,\r\n",
    "            self.max_num_elements,\r\n",
    "            self.distance_block_scalar_max,\r\n",
    "            self.distance_expansion_forces,\r\n",
    "            self.scale_distances,\r\n",
    "        )\r\n",
    "\r\n",
    "        self.message_blocks = ModuleList()\r\n",
    "        for _ in range(num_interactions):\r\n",
    "            block = MessageBlock(\r\n",
    "                hidden_channels,\r\n",
    "                hidden_channels,\r\n",
    "                mid_hidden_channels,\r\n",
    "                embedding_size,\r\n",
    "                self.sphere_size_lat,\r\n",
    "                self.sphere_size_long,\r\n",
    "                self.max_num_elements,\r\n",
    "                self.sphere_message,\r\n",
    "                self.act,\r\n",
    "                self.lmax,\r\n",
    "            )\r\n",
    "            self.message_blocks.append(block)\r\n",
    "\r\n",
    "        self.energyembeddingblock = EmbeddingBlock(\r\n",
    "            hidden_channels,\r\n",
    "            1,\r\n",
    "            mid_hidden_channels,\r\n",
    "            embedding_size,\r\n",
    "            8,\r\n",
    "            self.max_num_elements,\r\n",
    "            self.act,\r\n",
    "        )\r\n",
    "\r\n",
    "        if force_estimator == \"random\":\r\n",
    "            self.force_output_block = ForceOutputBlock(\r\n",
    "                hidden_channels,\r\n",
    "                2,\r\n",
    "                mid_hidden_channels,\r\n",
    "                embedding_size,\r\n",
    "                self.sphere_size_lat,\r\n",
    "                self.sphere_size_long,\r\n",
    "                self.max_num_elements,\r\n",
    "                self.output_message,\r\n",
    "                self.act,\r\n",
    "                self.lmax,\r\n",
    "            )\r\n",
    "\r\n",
    "    @conditional_grad(torch.enable_grad())\r\n",
    "    def forward(self, data):\r\n",
    "        self.device = data.pos.device\r\n",
    "        self.num_atoms = len(data.batch)\r\n",
    "        self.batch_size = len(data.natoms)\r\n",
    "        outputs = self._forward_helper(\r\n",
    "            data, edge_index, edge_distance, edge_distance_vec\r\n",
    "        )\r\n",
    "        if self.show_timing_info is True:\r\n",
    "            torch.cuda.synchronize()\r\n",
    "            print(\r\n",
    "                \"Memory: {}\\t{}\\t{}\".format(\r\n",
    "                    len(edge_index[0]),\r\n",
    "                    torch.cuda.memory_allocated()\r\n",
    "                    / (1000 * len(edge_index[0])),\r\n",
    "                    torch.cuda.max_memory_allocated() / 1000000,\r\n",
    "                )\r\n",
    "            )\r\n",
    "\r\n",
    "        return outputs\r\n",
    "\r\n",
    "    # restructure forward helper for conditional grad\r\n",
    "    def _forward_helper(\r\n",
    "        self, data, edge_index, edge_distance, edge_distance_vec\r\n",
    "    ):\r\n",
    "        ###############################################################\r\n",
    "        # Initialize messages\r\n",
    "        ###############################################################\r\n",
    "        #Заменить и сорс и таргет эмбеддинги на наши\r\n",
    "        source_element = data.atomic_numbers[edge_index[0, :]].long()\r\n",
    "        target_element = data.atomic_numbers[edge_index[1, :]].long()\r\n",
    "\r\n",
    "        x_dist = self.dist_block(edge_distance, source_element, target_element)\r\n",
    "\r\n",
    "        x = x_dist\r\n",
    "        x = self.distfc1(x)\r\n",
    "        x = self.act(x)\r\n",
    "        x = self.distfc2(x)\r\n",
    "        x = self.act(x)\r\n",
    "        x = self.embeddingblock2(x, source_element, target_element)\r\n",
    "\r\n",
    "        ###############################################################\r\n",
    "        # Update messages using block interactions\r\n",
    "        ###############################################################\r\n",
    "\r\n",
    "        edge_rot_mat = self._init_edge_rot_mat(\r\n",
    "            data, edge_index, edge_distance_vec\r\n",
    "        )\r\n",
    "        (\r\n",
    "            proj_edges_index,\r\n",
    "            proj_edges_delta,\r\n",
    "            proj_edges_src_index,\r\n",
    "        ) = self._project2D_edges_init(\r\n",
    "            edge_rot_mat, edge_index, edge_distance_vec\r\n",
    "        )\r\n",
    "\r\n",
    "        for block_index, interaction in enumerate(self.message_blocks):\r\n",
    "            x_out = interaction(\r\n",
    "                x,\r\n",
    "                x_dist,\r\n",
    "                source_element,\r\n",
    "                target_element,\r\n",
    "                proj_edges_index,\r\n",
    "                proj_edges_delta,\r\n",
    "                proj_edges_src_index,\r\n",
    "            )\r\n",
    "\r\n",
    "            if block_index > 0:\r\n",
    "                x = x + x_out\r\n",
    "            else:\r\n",
    "                x = x_out\r\n",
    "\r\n",
    "        ###############################################################\r\n",
    "        # Decoder\r\n",
    "        # Compute the forces and energies from the messages\r\n",
    "        ###############################################################\r\n",
    "        assert self.force_estimator in [\"random\", \"grad\"]\r\n",
    "\r\n",
    "        energy = scatter(x, edge_index[1], dim=0, dim_size=data.num_nodes) / (\r\n",
    "            self.max_num_neighbors / 2.0 + 1.0\r\n",
    "        )\r\n",
    "        atomic_numbers = data.atomic_numbers.long()\r\n",
    "        energy = self.energyembeddingblock(\r\n",
    "            energy, atomic_numbers, atomic_numbers\r\n",
    "        )\r\n",
    "        energy = scatter(energy, data.batch, dim=0)\r\n",
    "\r\n",
    "        if not self.regress_forces:\r\n",
    "            return energy\r\n",
    "        else:\r\n",
    "            return energy, forces\r\n",
    "\r\n",
    "    def _init_edge_rot_mat(self, data, edge_index, edge_distance_vec):\r\n",
    "        device = data.pos.device\r\n",
    "        num_atoms = len(data.batch)\r\n",
    "\r\n",
    "        edge_vec_0 = edge_distance_vec\r\n",
    "        edge_vec_0_distance = torch.sqrt(torch.sum(edge_vec_0 ** 2, dim=1))\r\n",
    "\r\n",
    "        if torch.min(edge_vec_0_distance) < 0.0001:\r\n",
    "            print(\r\n",
    "                \"Error edge_vec_0_distance: {}\".format(\r\n",
    "                    torch.min(edge_vec_0_distance)\r\n",
    "                )\r\n",
    "            )\r\n",
    "            (minval, minidx) = torch.min(edge_vec_0_distance, 0)\r\n",
    "            print(\r\n",
    "                \"Error edge_vec_0_distance: {} {} {} {} {}\".format(\r\n",
    "                    minidx,\r\n",
    "                    edge_index[0, minidx],\r\n",
    "                    edge_index[1, minidx],\r\n",
    "                    data.pos[edge_index[0, minidx]],\r\n",
    "                    data.pos[edge_index[1, minidx]],\r\n",
    "                )\r\n",
    "            )\r\n",
    "\r\n",
    "        avg_vector = torch.zeros(num_atoms, 3, device=device)\r\n",
    "        weight = 0.5 * (\r\n",
    "            torch.cos(edge_vec_0_distance * PI / self.cutoff) + 1.0\r\n",
    "        )\r\n",
    "        avg_vector.index_add_(\r\n",
    "            0, edge_index[1, :], edge_vec_0 * weight.view(-1, 1).expand(-1, 3)\r\n",
    "        )\r\n",
    "\r\n",
    "        edge_vec_2 = avg_vector[edge_index[1, :]] + 0.0001\r\n",
    "        edge_vec_2_distance = torch.sqrt(torch.sum(edge_vec_2 ** 2, dim=1))\r\n",
    "\r\n",
    "        if torch.min(edge_vec_2_distance) < 0.000001:\r\n",
    "            print(\r\n",
    "                \"Error edge_vec_2_distance: {}\".format(\r\n",
    "                    torch.min(edge_vec_2_distance)\r\n",
    "                )\r\n",
    "            )\r\n",
    "\r\n",
    "        norm_x = edge_vec_0 / (edge_vec_0_distance.view(-1, 1))\r\n",
    "        norm_0_2 = edge_vec_2 / (edge_vec_2_distance.view(-1, 1))\r\n",
    "        norm_z = torch.cross(norm_x, norm_0_2, dim=1)\r\n",
    "        norm_z = norm_z / (\r\n",
    "            torch.sqrt(torch.sum(norm_z ** 2, dim=1, keepdim=True)) + 0.0000001\r\n",
    "        )\r\n",
    "        norm_y = torch.cross(norm_x, norm_z, dim=1)\r\n",
    "        norm_y = norm_y / (\r\n",
    "            torch.sqrt(torch.sum(norm_y ** 2, dim=1, keepdim=True)) + 0.0000001\r\n",
    "        )\r\n",
    "\r\n",
    "        norm_x = norm_x.view(-1, 3, 1)\r\n",
    "        norm_y = norm_y.view(-1, 3, 1)\r\n",
    "        norm_z = norm_z.view(-1, 3, 1)\r\n",
    "\r\n",
    "        edge_rot_mat_inv = torch.cat([norm_x, norm_y, norm_z], dim=2)\r\n",
    "        edge_rot_mat = torch.transpose(edge_rot_mat_inv, 1, 2)\r\n",
    "\r\n",
    "        return edge_rot_mat\r\n",
    "\r\n",
    "    def _project2D_edges_init(self, rot_mat, edge_index, edge_distance_vec):\r\n",
    "        torch.set_printoptions(sci_mode=False)\r\n",
    "        length = len(edge_distance_vec)\r\n",
    "        device = edge_distance_vec.device\r\n",
    "\r\n",
    "        # Assuming the edges are consecutive based on the target index\r\n",
    "        target_node_index, neigh_count = torch.unique_consecutive(\r\n",
    "            edge_index[1], return_counts=True\r\n",
    "        )\r\n",
    "        max_neighbors = torch.max(neigh_count)\r\n",
    "        target_neigh_count = torch.zeros(self.num_atoms, device=device).long()\r\n",
    "        target_neigh_count.index_copy_(\r\n",
    "            0, target_node_index.long(), neigh_count\r\n",
    "        )\r\n",
    "\r\n",
    "        index_offset = (\r\n",
    "            torch.cumsum(target_neigh_count, dim=0) - target_neigh_count\r\n",
    "        )\r\n",
    "        neigh_index = torch.arange(length, device=device)\r\n",
    "        neigh_index = neigh_index - index_offset[edge_index[1]]\r\n",
    "\r\n",
    "        edge_map_index = edge_index[1] * max_neighbors + neigh_index\r\n",
    "        target_lookup = (\r\n",
    "            torch.zeros(self.num_atoms * max_neighbors, device=device) - 1\r\n",
    "        ).long()\r\n",
    "        target_lookup.index_copy_(\r\n",
    "            0,\r\n",
    "            edge_map_index.long(),\r\n",
    "            torch.arange(length, device=device).long(),\r\n",
    "        )\r\n",
    "        target_lookup = target_lookup.view(self.num_atoms, max_neighbors)\r\n",
    "\r\n",
    "        # target_lookup - For each target node, a list of edge indices\r\n",
    "        # target_neigh_count - number of neighbors for each target node\r\n",
    "        source_edge = target_lookup[edge_index[0]]\r\n",
    "        target_edge = (\r\n",
    "            torch.arange(length, device=device)\r\n",
    "            .long()\r\n",
    "            .view(-1, 1)\r\n",
    "            .repeat(1, max_neighbors)\r\n",
    "        )\r\n",
    "\r\n",
    "        source_edge = source_edge.view(-1)\r\n",
    "        target_edge = target_edge.view(-1)\r\n",
    "\r\n",
    "        mask_unused = source_edge.ge(0)\r\n",
    "        source_edge = torch.masked_select(source_edge, mask_unused)\r\n",
    "        target_edge = torch.masked_select(target_edge, mask_unused)\r\n",
    "\r\n",
    "        return self._project2D_init(\r\n",
    "            source_edge, target_edge, rot_mat, edge_distance_vec\r\n",
    "        )\r\n",
    "\r\n",
    "    def _project2D_init(\r\n",
    "        self, source_edge, target_edge, rot_mat, edge_distance_vec\r\n",
    "        ):\r\n",
    "        edge_distance_norm = F.normalize(edge_distance_vec)\r\n",
    "        source_edge_offset = edge_distance_norm[source_edge]\r\n",
    "\r\n",
    "        source_edge_offset_rot = torch.bmm(\r\n",
    "            rot_mat[target_edge], source_edge_offset.view(-1, 3, 1)\r\n",
    "        )\r\n",
    "\r\n",
    "        source_edge_X = torch.atan2(\r\n",
    "            source_edge_offset_rot[:, 1], source_edge_offset_rot[:, 2]\r\n",
    "        ).view(-1)\r\n",
    "\r\n",
    "        # source_edge_X ranges from -pi to pi\r\n",
    "        source_edge_X = (source_edge_X + math.pi) / (2.0 * math.pi)\r\n",
    "\r\n",
    "        # source_edge_Y ranges from -1 to 1\r\n",
    "        source_edge_Y = source_edge_offset_rot[:, 0].view(-1)\r\n",
    "        source_edge_Y = torch.clamp(source_edge_Y, min=-1.0, max=1.0)\r\n",
    "        source_edge_Y = (source_edge_Y.asin() + (math.pi / 2.0)) / (\r\n",
    "            math.pi\r\n",
    "        )  # bin by angle\r\n",
    "        # source_edge_Y = (source_edge_Y + 1.0) / 2.0 # bin by sin\r\n",
    "        source_edge_Y = 0.99 * (source_edge_Y) + 0.005\r\n",
    "\r\n",
    "        source_edge_X = source_edge_X * self.sphere_size_long\r\n",
    "        source_edge_Y = source_edge_Y * (\r\n",
    "            self.sphere_size_lat - 1.0\r\n",
    "        )  # not circular so pad by one\r\n",
    "\r\n",
    "        source_edge_X_0 = torch.floor(source_edge_X).long()\r\n",
    "        source_edge_X_del = source_edge_X - source_edge_X_0\r\n",
    "        source_edge_X_0 = source_edge_X_0 % self.sphere_size_long\r\n",
    "        source_edge_X_1 = (source_edge_X_0 + 1) % self.sphere_size_long\r\n",
    "\r\n",
    "        source_edge_Y_0 = torch.floor(source_edge_Y).long()\r\n",
    "        source_edge_Y_del = source_edge_Y - source_edge_Y_0\r\n",
    "        source_edge_Y_0 = source_edge_Y_0 % self.sphere_size_lat\r\n",
    "        source_edge_Y_1 = (source_edge_Y_0 + 1) % self.sphere_size_lat\r\n",
    "\r\n",
    "        # Compute the values needed to bilinearly splat the values onto the spheres\r\n",
    "        index_0_0 = (\r\n",
    "            target_edge * self.sphere_size_lat * self.sphere_size_long\r\n",
    "            + source_edge_Y_0 * self.sphere_size_long\r\n",
    "            + source_edge_X_0\r\n",
    "        )\r\n",
    "        index_0_1 = (\r\n",
    "            target_edge * self.sphere_size_lat * self.sphere_size_long\r\n",
    "            + source_edge_Y_0 * self.sphere_size_long\r\n",
    "            + source_edge_X_1\r\n",
    "        )\r\n",
    "        index_1_0 = (\r\n",
    "            target_edge * self.sphere_size_lat * self.sphere_size_long\r\n",
    "            + source_edge_Y_1 * self.sphere_size_long\r\n",
    "            + source_edge_X_0\r\n",
    "        )\r\n",
    "        index_1_1 = (\r\n",
    "            target_edge * self.sphere_size_lat * self.sphere_size_long\r\n",
    "            + source_edge_Y_1 * self.sphere_size_long\r\n",
    "            + source_edge_X_1\r\n",
    "        )\r\n",
    "\r\n",
    "        delta_0_0 = (1.0 - source_edge_X_del) * (1.0 - source_edge_Y_del)\r\n",
    "        delta_0_1 = (source_edge_X_del) * (1.0 - source_edge_Y_del)\r\n",
    "        delta_1_0 = (1.0 - source_edge_X_del) * (source_edge_Y_del)\r\n",
    "        delta_1_1 = (source_edge_X_del) * (source_edge_Y_del)\r\n",
    "\r\n",
    "        index_0_0 = index_0_0.view(1, -1)\r\n",
    "        index_0_1 = index_0_1.view(1, -1)\r\n",
    "        index_1_0 = index_1_0.view(1, -1)\r\n",
    "        index_1_1 = index_1_1.view(1, -1)\r\n",
    "\r\n",
    "        # NaNs otherwise\r\n",
    "        if self.grad_forces:\r\n",
    "            with torch.no_grad():\r\n",
    "                delta_0_0 = delta_0_0.view(1, -1)\r\n",
    "                delta_0_1 = delta_0_1.view(1, -1)\r\n",
    "                delta_1_0 = delta_1_0.view(1, -1)\r\n",
    "                delta_1_1 = delta_1_1.view(1, -1)\r\n",
    "        else:\r\n",
    "            delta_0_0 = delta_0_0.view(1, -1)\r\n",
    "            delta_0_1 = delta_0_1.view(1, -1)\r\n",
    "            delta_1_0 = delta_1_0.view(1, -1)\r\n",
    "            delta_1_1 = delta_1_1.view(1, -1)\r\n",
    "\r\n",
    "        return (\r\n",
    "            torch.cat([index_0_0, index_0_1, index_1_0, index_1_1]),\r\n",
    "            torch.cat([delta_0_0, delta_0_1, delta_1_0, delta_1_1]),\r\n",
    "            source_edge,\r\n",
    "        )\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "#model\r\n",
    "model = ConvNN(dim_atom=training_set[0][0]['x'].shape[1], dim_edge=training_set[0][0]['edge_attr'].shape[1])\r\n",
    "\r\n",
    "#optimizer and loss\r\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\r\n",
    "criterion = nn.L1Loss()\r\n",
    "\r\n",
    "#переносим на куду если она есть\r\n",
    "model = model.to('cpu')\r\n",
    "criterion = criterion.to('cpu')\r\n",
    "\r\n",
    "logfile_str = {\r\n",
    "    \"lr\": \"0.001\",\r\n",
    "    \"smearing\" : \"smearing\"\r\n",
    "}\r\n",
    "\r\n",
    "#граф модели\r\n",
    "trace_system = dict(list(next(iter(training_generator))[0]))\r\n",
    "writer.add_graph(model, trace_system)\r\n",
    "writer.add_text(timestamp, str(logfile_str))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class GConv(MessagePassing):\r\n",
    "    def __init__(self):\r\n",
    "        super(GConv, self).__init__(aggr='add')  # \"Add\" aggregation\r\n",
    "\r\n",
    "    def forward(self, batch):\r\n",
    "        x = batch['x']\r\n",
    "        edge_index = batch['edge_index']\r\n",
    "        edge_attr = batch['edge_attr']\r\n",
    "        \r\n",
    "        # x has shape [N -- количество атомов в системе(батче), in_channels -- размерность вектора-атома]\r\n",
    "        # edge_index has shape [2, E] -- каждое ребро задаётся парой вершин\r\n",
    "\r\n",
    "        # Start propagating messages. \r\n",
    "    \r\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr, size=None)\r\n",
    "\r\n",
    "    def message(self, x, x_i, x_j, edge_attr):\r\n",
    "        # your function\r\n",
    "        pass\r\n",
    "        \r\n",
    "    def update(self, aggr_out, x, edge_attr, edge_index):        \r\n",
    "        #your function\r\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ConvNN(nn.Module):\r\n",
    "    \r\n",
    "    def __init__(self):\r\n",
    "        \r\n",
    "        super().__init__()          \r\n",
    "        self.conv = GConv()\r\n",
    "        \r\n",
    "    def forward(self, batch):\r\n",
    "        #your function\r\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#config\r\n",
    "batch_size = 50\r\n",
    "num_workers = 0\r\n",
    "\r\n",
    "features_cols = ['atomic_numbers', 'edge_index_new', 'distances_new', \r\n",
    "                 'contact_solid_angles', 'tags', 'voronoi_volumes', 'spherical_domain_radii']\r\n",
    "\r\n",
    "target_col = 'y_relaxed'\r\n",
    "lr = 0.001\r\n",
    "epochs = 20"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#чтобы тензор по умолчанию заводился на куде\r\n",
    "if torch.cuda.is_available():\r\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\r\n",
    "    print('cuda')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#set device\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \r\n",
    "print(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#инициализируем тренировочный датасети и тренировочный итератор\r\n",
    "train_dataset_file_path= os.path.expanduser(\"../../ocp_datasets/data/is2re/10k/train/data_mod2.lmdb\")\r\n",
    "\r\n",
    "training_set = Dataset(train_dataset_file_path, features_cols, target_col, preprocessing=preprocessing)\r\n",
    "training_generator = DataLoader(training_set, batch_size=batch_size, num_workers=num_workers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#инициализируем валидационный датасет и валидационный итератор\r\n",
    "val_dataset_file_path = os.path.expanduser(\"../../ocp_datasets/data/is2re/all/val_ood_both/data_mod2.lmdb\")\r\n",
    "\r\n",
    "valid_set = Dataset(val_dataset_file_path, features_cols, target_col, preprocessing=preprocessing)\r\n",
    "valid_generator = DataLoader(valid_set, batch_size=batch_size, num_workers=num_workers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "try:\r\n",
    "    lmdb_dataset(train_dataset_file_path).describe()\r\n",
    "except:\r\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#model\r\n",
    "model = ConvNN(dim_atom=training_set[0][0]['x'].shape[1], dim_edge=training_set[0][0]['edge_attr'].shape[1])\r\n",
    "\r\n",
    "#optimizer and loss\r\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\r\n",
    "criterion = nn.L1Loss()\r\n",
    "\r\n",
    "#переносим на куду если она есть\r\n",
    "model = model.to(device)\r\n",
    "criterion = criterion.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "timestamp = str(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\r\n",
    "\r\n",
    "print(timestamp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#tensorboard writer, при первом запуске надо руками сделать папку для логов\r\n",
    "\r\n",
    "# server\r\n",
    "#log_folder_path = \"../../ocp_results/logs/tensorboard/out_base_model\"\r\n",
    "\r\n",
    "# colab\r\n",
    "# log_folder_path = \"/content/drive/MyDrive/ocp_results/logs/tensorboard/out_base_model\"\r\n",
    "\r\n",
    "# user_specific \r\n",
    "log_file_path = \"../logs/tensorboard_airi\"\r\n",
    "\r\n",
    "writer = SummaryWriter(log_file_path + '/' + timestamp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "logfile_str = {\r\n",
    "    \"train_dataset_file_path\": train_dataset_file_path,\r\n",
    "    \"val_dataset_file_path\": val_dataset_file_path,\r\n",
    "    \"features_cols\": features_cols,\r\n",
    "    \"target_col\": target_col,\r\n",
    "    \"batch_size\": batch_size,\r\n",
    "    \"num_workers\": num_workers,\r\n",
    "    \"epochs\": epochs,\r\n",
    "    \"lr\": lr\r\n",
    "}\r\n",
    "\r\n",
    "#граф модели\r\n",
    "trace_system = dict(list(next(iter(training_generator))[0]))\r\n",
    "writer.add_graph(model, trace_system)\r\n",
    "writer.add_text(timestamp, str(logfile_str))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "loss = []\r\n",
    "loss_eval = []\r\n",
    "\r\n",
    "print(timestamp)\r\n",
    "print(f'Start training model {str(model)}')\r\n",
    "for i in range(epochs):\r\n",
    "    loss.append(train(model, training_generator, optimizer, criterion, epoch=i, writer=writer, device=device))\r\n",
    "    loss_eval.append(evaluate(model, valid_generator, criterion, epoch=i, writer=writer, device=device))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.12 64-bit ('ocp_models': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "interpreter": {
   "hash": "fab7f7c0ded9c666ce2eedf60bd1f674753dedfe88843c732645b3b92d2549e6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}