{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee56f789-c65f-496b-b249-f0f892f31f17",
   "metadata": {},
   "source": [
    "# Modification of the dataset data.lmdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d48b62-66d4-4ba0-9b3d-106f68d31c2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Voronoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c041ec25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ocpmodels.datasets import SinglePointLmdbDataset\n",
    "\n",
    "import numpy as np\n",
    "from mendeleev import element\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.spatial.distance import euclidean\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import copy\n",
    "import torch\n",
    "import lmdb\n",
    "import re\n",
    "\n",
    "def getAtomSequence (sequence) :\n",
    "    result = list([[sequence[0], 1]])\n",
    "    for i in range(1, len(sequence)) :\n",
    "        if sequence[i] == result[-1][0] :\n",
    "            result[-1][1] += 1\n",
    "        else :\n",
    "            result.append([sequence[i], 1])\n",
    "    return dict(result)\n",
    "\n",
    "def structureToVASP(structure, file='POSCAR', str_name='structure', relaxed=False) :\n",
    "    with open(file + ('_relaxed' if relaxed else ''), 'w') as f :\n",
    "        f.write(str_name + '\\n')\n",
    "        f.write(str(1.0) + '\\n')\n",
    "        for axis in np.array(structure.cell[0]) :\n",
    "            for i in range(3) :\n",
    "                f.write(str(axis[i]) + '   ')\n",
    "                if i == 2 :\n",
    "                    f.write('\\n')\n",
    "        atoms = getAtomSequence(np.array(structure.atomic_numbers, dtype=int))\n",
    "        for k in atoms.keys() :\n",
    "            f.write('   ' + element(round(k)).symbol)\n",
    "        f.write('\\n')\n",
    "        for v in atoms.values() :\n",
    "            f.write('   ' + str(round(v)))\n",
    "        f.write('\\n')        \n",
    "        f.write('Cartesian\\n')\n",
    "        for position in np.array(structure.pos if not relaxed else structure.pos_relaxed)   :\n",
    "            for i in range(3) :\n",
    "                f.write(str(position[i]) + '   ')\n",
    "                if i == 2 :\n",
    "                    f.write('\\n')            \n",
    "    return None\n",
    "\n",
    "def getTranslations (positions, cell) :\n",
    "    result = np.array(positions)\n",
    "    #print(result.shape)\n",
    "    for i in range(-1,2) :\n",
    "        for j in range(-1,2) :\n",
    "            for k in range(-1,2) :\n",
    "                if (i == 0) and (j == 0) and (k == 0) :\n",
    "                    continue\n",
    "                result = np.vstack((result, np.array(positions) + \n",
    "                                    i*np.array(cell[0][0]) + \n",
    "                                    j*np.array(cell[0][1]) +\n",
    "                                    k*np.array(cell[0][2])) )\n",
    "    return result\n",
    "\n",
    "def getOffsets (positions) :\n",
    "    result = np.zeros_like(positions, dtype=int)\n",
    "    #print(result.shape)\n",
    "    for i in range(-1,2) :\n",
    "        for j in range(-1,2) :\n",
    "            for k in range(-1,2) :\n",
    "                if (i == 0) and (j == 0) and (k == 0) :\n",
    "                    continue\n",
    "                result = np.vstack((result, np.zeros_like(positions, dtype=int) + [i,j,k]) )\n",
    "    return result\n",
    "\n",
    "import functions, polyhedron, graph, geometry\n",
    "from scipy.spatial import Voronoi\n",
    "from polyhedron import Polyhedron\n",
    "import geometry as gm\n",
    "import numpy as np\n",
    "\n",
    " \n",
    "class Voro:\n",
    "\n",
    "    def __init__(self, points, central_ps, labels):\n",
    "        \"\"\"\n",
    "        :param points: list of points coordinates, [[float, float, float], ...]\n",
    "        :param central_ps: indexes of centrals points, list of int\n",
    "        :param labels: list of labels\n",
    "        \"\"\"\n",
    "        self.points = np.array(points)\n",
    "        self.central_ps = central_ps\n",
    "        self.labels = labels\n",
    "        self.vor = Voronoi(points)\n",
    "        self.p_adjacency = self.calc_p_adjacency()\n",
    "        self.polyhedrons = self.construct_polyhedrons()\n",
    "        self.rsds = np.array([(3 * p.volume / (4 * np.pi)) ** (1 / 3.) for p in self.polyhedrons])\n",
    "        self.angles = self.calc_angles()\n",
    "        self.direct_neighbors = self.find_direct_neighbors()\n",
    "\n",
    "    def calc_p_adjacency(self):\n",
    "        \"\"\"\n",
    "        Calculation of points adjacency list (points are adjacent if the domains of the points are adjacent)\n",
    "        :return: self.p_adjacency: points adjacency list, [[int, ...], ...]\n",
    "        \"\"\"\n",
    "        p_adjacency = [[] for _ in range(len(self.points))]\n",
    "        for p1, p2 in self.vor.ridge_dict.keys():\n",
    "            p_adjacency[p1] += [p2]\n",
    "            p_adjacency[p2] += [p1]\n",
    "        self.p_adjacency = p_adjacency\n",
    "        return self.p_adjacency\n",
    "\n",
    "    def construct_polyhedrons(self):\n",
    "        \"\"\"\n",
    "        Construct of the polyhedrons\n",
    "        :return: self.polyhedrons, Voronoi polyhedra, list of Polyhedron objects\n",
    "        \"\"\"\n",
    "        self.polyhedrons = []\n",
    "        for i in self.central_ps:\n",
    "            faces = []\n",
    "            region = self.vor.regions[self.vor.point_region[i]]\n",
    "            new_ind = {o_i: n_i for n_i, o_i in enumerate(region)}\n",
    "            if -1 in region:\n",
    "                raise RuntimeError(\"The domain for \\\"\" + str(i) + \"\\\" point is not closed!\")\n",
    "            for j in self.p_adjacency[i]:\n",
    "                common_vs = self.vor.ridge_dict.get((i, j))\n",
    "                if common_vs is None:\n",
    "                    common_vs = self.vor.ridge_dict[(j, i)][::-1]\n",
    "                if i != j and common_vs is not None and len(common_vs) > 2:\n",
    "                    faces += [[new_ind[o_i] for o_i in common_vs]]\n",
    "                else:\n",
    "                    raise RuntimeError(\"The Voronoi decomposition is failed!\")\n",
    "            faces = np.array([np.array(f) for f in faces])\n",
    "            self.polyhedrons += [Polyhedron(self.vor.vertices[region], region, faces, find_order=False)]\n",
    "        return self.polyhedrons\n",
    "\n",
    "    def calc_angles(self):\n",
    "        \"\"\"\n",
    "        Calculation of solid angles\n",
    "        :return: self.angles, solid angles between adjacent Voronoi polyhedra, {(int, int): float, ...}\n",
    "        \"\"\"\n",
    "        self.angles = {}\n",
    "        for i, i1 in enumerate(self.central_ps):\n",
    "            angles = []\n",
    "            cp = self.points[i1]\n",
    "            for j, i2 in enumerate(self.p_adjacency[i1]):\n",
    "                angles += [sum([abs(gm.calc_solid_angle(cp, s)) for s in self.polyhedrons[i].faces[j].simplexes])]\n",
    "            angles = 100 * np.array(angles) / sum(angles)\n",
    "            for j, i2 in enumerate(self.p_adjacency[i1]):\n",
    "                self.angles[(i1, i2)] = angles[j]\n",
    "                self.angles[(i2, i1)] = angles[j]\n",
    "        return self.angles\n",
    "\n",
    "    def find_direct_neighbors(self):\n",
    "        \"\"\"\n",
    "        Find the direct neighbors\n",
    "        :return: self.direct_neighbors: direct Voronoi polyhedra neighbors, {(int, int): bool}\n",
    "        \"\"\"\n",
    "        self.direct_neighbors = {}\n",
    "        for i, i1 in enumerate(self.central_ps):\n",
    "            p1 = self.points[i1]\n",
    "            for j, i2 in enumerate(self.p_adjacency[i1]):\n",
    "                p2 = self.points[i2]\n",
    "                if self.polyhedrons[i].faces[j].is_inside(gm.calc_centroid([p1, p2])):\n",
    "                    self.direct_neighbors[(i1, i2)] = True\n",
    "                    self.direct_neighbors[(i2, i1)] = True\n",
    "        return self.direct_neighbors\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0fd470-8378-4e82-83e6-e6c37534a37f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## New properties 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb552ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newProperties(data) :\n",
    "    keys = []\n",
    "    result = []\n",
    "    \n",
    "    points = getTranslations(data.pos, data.cell)\n",
    "    atom_index = np.array(list(range(data.natoms)) * round(len(points) / len(data.pos)))\n",
    "    offsets = getOffsets(data.pos)\n",
    "    voro = Voro(points, range(data.natoms), list(range(len(points))))\n",
    "    \n",
    "    volumes = np.array(list(map(lambda x: x.volume, voro.polyhedrons)))\n",
    "    surface_areas = np.array(list(map(lambda x: x.area, voro.polyhedrons)))\n",
    "    rsds = voro.rsds\n",
    "    \n",
    "    keys.extend(['voronoi_volumes', 'voronoi_surface_areas',\n",
    "                'spherical_domain_radii'])\n",
    "    result.extend([volumes, surface_areas, rsds])    \n",
    "    \n",
    "    df = pd.DataFrame(voro.angles.keys(), columns=['VA_p1', 'VA_p2'])\n",
    "    #print(df)\n",
    "    df['cell_offsets'] = list(map(tuple, offsets[df['VA_p1']] - offsets[df['VA_p2']])) \n",
    "    #sign correspond to original data\n",
    "    df['VA_p1_corr'] = atom_index[df['VA_p1']]\n",
    "    df['VA_p2_corr'] = atom_index[df['VA_p2']]\n",
    "    df['direct_neighbor'] = list(map(lambda x: 1 if x in voro.direct_neighbors.keys() else 0, voro.angles.keys()))\n",
    "    k = np.array(list(voro.angles.keys()))\n",
    "    df['distance'] = np.array([euclidean(points[pair[0]], points[pair[1]]) for pair in k])\n",
    "    df['solid_angle'] = voro.angles.values()\n",
    "    df['to_keep'] = ~df.duplicated(subset=['VA_p1_corr', 'VA_p2_corr', 'cell_offsets'], keep='first')\n",
    "    df = df[df['to_keep']].drop(labels=['to_keep', 'VA_p1', 'VA_p2'], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    keys.extend(['cell_offsets_new', 'distances_new', 'contact_solid_angles',\n",
    "                'direct_neighbor', 'edge_index_new'])\n",
    "    result.extend([np.array(list(map(np.array, df['cell_offsets'].values))), df['distance'].values, \n",
    "                   df['solid_angle'].values, df['direct_neighbor'].values, \n",
    "                   df[['VA_p1_corr', 'VA_p2_corr']].values.T])  \n",
    "    \n",
    "    return dict(zip(keys, result))\n",
    "\n",
    "def processDataset (dataset_path, dataset_batch_size=5004) :\n",
    "    dataset_path_modified = re.sub('\\.lmdb', '_mod.lmdb', dataset_path)\n",
    "    print('###########################################')\n",
    "    print(dataset_path_modified)\n",
    "    print()\n",
    "    \n",
    "    dataset = SinglePointLmdbDataset({\"src\": dataset_path})\n",
    "    print('Original dataset size is {}'.format(len(dataset)))\n",
    "    print()\n",
    "    \n",
    "    dataset_target = lmdb.open(\n",
    "        dataset_path_modified,\n",
    "        map_size=int(1e9*50), #~ 50 Gbyte\n",
    "        subdir=False,\n",
    "        meminit=False,\n",
    "        map_async=True,)\n",
    "    \n",
    "    print('Batch info:')\n",
    "    \n",
    "    for batch in range(len(dataset) // dataset_batch_size + \n",
    "                       (1 if len(dataset) % dataset_batch_size != 0 else 0)) :\n",
    "        print('{}: from {} to {}'.format(batch, batch * dataset_batch_size, \n",
    "                                         min(len(dataset), (batch+1) * dataset_batch_size) - 1))\n",
    "    \n",
    "    print()\n",
    "    for batch in range(len(dataset) // dataset_batch_size + \n",
    "                       (1 if len(dataset) % dataset_batch_size != 0 else 0)) :\n",
    "        \n",
    "        dataset_under_process = [dataset[i] for i in \n",
    "                                 range(batch * dataset_batch_size, \n",
    "                                       min(len(dataset), (batch+1) * dataset_batch_size))]\n",
    "        print('Structures from {} to {} are under process...'.format(batch * dataset_batch_size, \n",
    "                                                batch * dataset_batch_size + len(dataset_under_process) - 1))\n",
    "        \n",
    "        res = Parallel(n_jobs=-1)(delayed(newProperties)(dataset_under_process[i]) \n",
    "                                  for i in tqdm(range(len(dataset_under_process))))\n",
    "        #res = [dict() for i in range(len(dataset_under_process))]\n",
    "        print('are stored to a file...')\n",
    "        \n",
    "        for structure_id in tqdm(range(len(dataset_under_process))) :\n",
    "            txn = dataset_target.begin(write=True)\n",
    "            data = dataset_under_process[structure_id]\n",
    "            \n",
    "            for new_data_key in res[0].keys() :\n",
    "                #print(new_data_key)\n",
    "                data[new_data_key] = torch.from_numpy(res[structure_id][new_data_key])\n",
    "                #print(data[new_data_key])\n",
    "            txn.put(f\"{structure_id + batch * dataset_batch_size}\".encode(\"ascii\"), \n",
    "                    pickle.dumps(data, protocol=-1))\n",
    "            txn.commit()\n",
    "            dataset_target.sync()\n",
    "    \n",
    "    dataset_target.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa7efe5-a6ed-409a-a254-642dcf7e8434",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## New properties 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34232c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newProperties2(data) :\n",
    "    ei = data['edge_index_new'].numpy()\n",
    "    di = data['distances_new'].numpy()\n",
    "    dn = data['direct_neighbor'].numpy()\n",
    "    co = data['cell_offsets_new'].numpy()\n",
    "    co = list(map(tuple, co))\n",
    "\n",
    "    # тут можно добавить фильтры на связность (по ближайшим соседям или по максимальным длинам)\n",
    "\n",
    "    Nedges = di.shape[0]\n",
    "    #print(dict(enumerate(co)),)\n",
    "    res_even = [calcAnglesForEdge(edgeN, dict(enumerate(ei.T)), dict(enumerate(di)), dict(enumerate(co)), \n",
    "                                  data['pos'].numpy(), data['cell'].numpy()) for edgeN in range(Nedges)[::2]]\n",
    "    # accounting for inverse edges\n",
    "    res_odd = copy.deepcopy(res_even)\n",
    "    for i in range(len(res_odd)) :\n",
    "        res_odd[i]['edge_theta'] *= -1 \n",
    "        res_odd[i]['edge_theta'] += np.pi\n",
    "        res_odd[i]['edge_phi'] *= -1\n",
    "\n",
    "    res = []\n",
    "    for i in range(len(res_odd)) :\n",
    "        res += [res_even[i]] + [res_odd[i]]\n",
    "    return {'edge_angles' : res}\n",
    "        \n",
    "\n",
    "def calcAnglesForEdge(edgeN, edge_by_edgeN, distance_by_edgeN, cell_offset_by_edgeN, positions, cell) :\n",
    "    edge = edge_by_edgeN[edgeN]\n",
    "    edge_positions = np.vstack([positions[edge[i]] for i in range(2)])\n",
    "    #print('EP', edge_positions)\n",
    "    #print(edgeN, len(edge_by_edgeN), len(distance_by_edgeN), len(cell_offset_by_edgeN), positions, cell)\n",
    "    edge_positions[1] -= np.matmul(cell_offset_by_edgeN[edgeN], cell)[0] # update the second position with offset\n",
    "    centroid = np.mean(edge_positions, axis=0)\n",
    "    #print('centroid', centroid)\n",
    "    v0 = edge_positions[1] - edge_positions[0] #edge vector\n",
    "    #print(v0)\n",
    "    v0 = v0/np.linalg.norm(v0) #edge unit direction\n",
    "    #searching for the neighbors of the edge\n",
    "    edge_neighborsN = [p for p in edge_by_edgeN.keys() if any([edge_by_edgeN[p][0] == edge[i] \n",
    "                                                               for i in range(2 if edge[0] != edge[1] else 1)]) ]\n",
    "    #print(edge_neighborsN)\n",
    "    edge_neighborsN.remove(edgeN) #remove edge itself\n",
    "    edge_neighborsN.remove(edgeN + 1) #remove edge invert to the given\n",
    "    #print(edge_neighborsN)\n",
    "    df = pd.DataFrame([[*edge_by_edgeN[i], distance_by_edgeN[i], cell_offset_by_edgeN[i]] \n",
    "                          for i in edge_neighborsN], columns=['start', 'end', 'distance','cell_offset'])\n",
    "    df['edge_id'] = edge_neighborsN\n",
    "    #df['to_keep'] = ~df.duplicated(subset=['end', 'cell_offset'])\n",
    "    #df = df[df['to_keep']].drop('to_keep', axis=1)\n",
    "   \n",
    "    result_theta = []\n",
    "    result_angle_to_z = []\n",
    "    plane_proj = [] #to evaluate phi of the neighboring edges further\n",
    "    #centroid2neighbor_dist = []\n",
    "    for i in range(2 if edge[0] != edge[1] else 1): # loop over the nodes of edge\n",
    "        centroid2node_vec = edge_positions[i] - centroid\n",
    "        for NES, NEE, offset in df[(df['start'] == edge[i])][['start', 'end', 'cell_offset']].values:\n",
    "            neighbor_edge = (NES, NEE)\n",
    "            neighbor_edge_positions = np.vstack([positions[neighbor_edge[i]] for i in range(2)])\n",
    "            neighbor_edge_positions[1] -= np.matmul(np.array(offset), cell)[0] \n",
    "            node2neighbor_vec = neighbor_edge_positions[1] - neighbor_edge_positions[0]\n",
    "            v1 = node2neighbor_vec + centroid2node_vec #centroid2neighbor_vec\n",
    "            #centroid2neighbor_dist += [np.linalg.norm(v1)]\n",
    "            #print(v1)\n",
    "            #print(v1, v0, np.dot(v0, v1))\n",
    "            plane_proj += [(v1 - v0 * np.dot(v0, v1))] #centroid2neighbor_vec proj on edge normal plane\n",
    "            v1 = v1/np.linalg.norm(v1) #centroid2neighbor unit direction\n",
    "            #print(v1)\n",
    "            result_theta += [np.arccos(np.clip(np.dot(v0, v1), -1, 1))]\n",
    "            result_angle_to_z += [np.arccos(np.clip(np.dot(np.array([0,0,1]), v1), -1, 1))]\n",
    "        #print('loop is over')\n",
    "    #print(edgeN, df.shape, len(result_theta), len(result_angle_to_z))\n",
    "    #df['cent2neigh_dist'] = centroid2neighbor_dist\n",
    "    df['edge_theta'] = result_theta\n",
    "    df['edge_to_z'] = result_angle_to_z\n",
    "    #print(np.argmin(df['distance']))\n",
    "    v1 = plane_proj[np.argmin(np.abs(df['edge_theta'] - np.pi/2))] # proj of centroid2most_incline neighbor \n",
    "    #if np.abs(np.linalg.norm(v1)) < 1e-7 :\n",
    "    #    print(v1, np.linalg.norm(v1), np.argmin(df['cent2neigh_dist']))\n",
    "    #    print(df)\n",
    "    v1 = v1/np.linalg.norm(v1)                  # unit direction\n",
    "    v2 = np.cross(v0, v1)                       # third axis\n",
    "    Rm = np.vstack([v1, v2, v0]).T              # rotation matrix\n",
    "    # rotation matrix transforms standard xyz CS to rotated z' -> edge direction, x' -> nearest neighbor\n",
    "    #print(Rm)\n",
    "    invRm = np.linalg.inv(Rm)                   # inverse rotation matrix \n",
    "    #print(invRm)\n",
    "    plane_proj_rot = np.array(list(map(lambda x: np.matmul(invRm, np.array(x).reshape(-1,1)).reshape(1,-1)[0], \n",
    "                                  plane_proj)))\n",
    "    #print(plane_proj_rot)\n",
    "    df['edge_phi'] = list(map(lambda x: np.angle(complex(x[0], x[1])), plane_proj_rot))\n",
    "    \n",
    "    return df[['edge_id', 'edge_theta', 'edge_to_z', 'edge_phi']].set_index('edge_id')\n",
    "\n",
    "def processDataset2 (dataset_path, dataset_batch_size=5004) :\n",
    "    dataset_path_modified = re.sub('_mod\\.lmdb', '_mod2.lmdb', dataset_path)\n",
    "    print('###########################################')\n",
    "    print(dataset_path_modified)\n",
    "    print()\n",
    "    \n",
    "    dataset = SinglePointLmdbDataset({\"src\": dataset_path})\n",
    "    #dataset = [dataset[i] for i in range(0, 12)]\n",
    "    print('Original dataset size is {}'.format(len(dataset)))\n",
    "    print()\n",
    "    \n",
    "    dataset_target = lmdb.open(\n",
    "        dataset_path_modified,\n",
    "        map_size=int(1e9*50), #~ 50 Gbyte\n",
    "        subdir=False,\n",
    "        meminit=False,\n",
    "        map_async=True,)\n",
    "    \n",
    "    print('Batch info:')\n",
    "    \n",
    "    for batch in range(len(dataset) // dataset_batch_size + \n",
    "                       (1 if len(dataset) % dataset_batch_size != 0 else 0)) :\n",
    "        print('{}: from {} to {}'.format(batch, batch * dataset_batch_size, \n",
    "                                         min(len(dataset), (batch+1) * dataset_batch_size) - 1))\n",
    "    \n",
    "    print()\n",
    "    for batch in range(len(dataset) // dataset_batch_size + \n",
    "                       (1 if len(dataset) % dataset_batch_size != 0 else 0)) :\n",
    "        \n",
    "        dataset_under_process = [dataset[i] for i in \n",
    "                                 range(batch * dataset_batch_size, \n",
    "                                       min(len(dataset), (batch+1) * dataset_batch_size))]\n",
    "        print('Structures from {} to {} are under process...'.format(batch * dataset_batch_size, \n",
    "                                                batch * dataset_batch_size + len(dataset_under_process) - 1))\n",
    "        \n",
    "        \n",
    "        \n",
    "        res = Parallel(n_jobs=-1)(delayed(newProperties2)(dataset_under_process[i]) \n",
    "                                  for i in tqdm(range(len(dataset_under_process))))\n",
    "        #res = [dict() for i in range(len(dataset_under_process))]\n",
    "        print('are stored to a file...')\n",
    "        \n",
    "        for structure_id in tqdm(range(len(dataset_under_process))) :\n",
    "            txn = dataset_target.begin(write=True)\n",
    "            data = dataset_under_process[structure_id]\n",
    "            \n",
    "            for new_data_key in res[0].keys() :\n",
    "                #print(new_data_key)\n",
    "                data[new_data_key] = res[structure_id][new_data_key]\n",
    "                #print(data[new_data_key])\n",
    "            txn.put(f\"{structure_id + batch * dataset_batch_size}\".encode(\"ascii\"), \n",
    "                    pickle.dumps(data, protocol=-1))\n",
    "            txn.commit()\n",
    "            dataset_target.sync()\n",
    "    \n",
    "    dataset_target.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c8285f-a461-4e83-b4d9-1c04df0958ce",
   "metadata": {},
   "source": [
    "## Proccesing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1c95b-c212-403a-9ab5-9fa7822d3076",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Process data.lmdb to data_mod.lmdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b49bb4-b2fe-4ba8-8842-aa8623381f1e",
   "metadata": {},
   "source": [
    "**Added feautures.** </br>\n",
    "atom: \\['voronoi_volumes', 'voronoi_surface_areas', 'spherical_domain_radii'], </br>\n",
    "bond; \\['cell_offsets_new', 'distances_new', 'contact_solid_angles', 'direct_neighbor', 'edge_index_new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b7b3c-d4b1-4271-a750-b71bceb35d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processDataset('/Users/Eremin/OneDrive/Share/10k/train/data.lmdb')\n",
    "#processDataset('/Users/Eremin/OneDrive/Share/all/val_ood_both/data.lmdb')\n",
    "#processDataset('/Users/Eremin/OneDrive/Share/all/test_ood_both/data.lmdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bff057-889d-4287-b204-a2604de41811",
   "metadata": {},
   "source": [
    "### Process data_mod.lmdb to data_mod2.lmdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb9165-f7b2-481e-8b76-3ff77717c38e",
   "metadata": {},
   "source": [
    "**Added feautures.** </br>\n",
    "angles: \\['edge_id', 'edge_theta', 'edge_to_z', 'edge_phi'], </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea0cdd-db7a-4402-8397-5501fdaffa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../../../ocp_datasets/data/is2re/all/val_ood_both/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afdaa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "processDataset2(dataset_path + 'data_mod.lmdb' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e181b03-959e-4c4c-a9fe-c904772bc886",
   "metadata": {},
   "source": [
    "**Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad81998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori = SinglePointLmdbDataset({\"src\": dataset_path + 'data_mod2.lmdb'})\n",
    "data_ori[0].keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7b95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori[0]['edge_angles'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocp_models",
   "language": "python",
   "name": "ocp_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
