{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c041ec25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ocpmodels.datasets import SinglePointLmdbDataset\n",
    "\n",
    "import numpy as np\n",
    "from mendeleev import element\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.spatial.distance import euclidean\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def getAtomSequence (sequence) :\n",
    "    result = list([[sequence[0], 1]])\n",
    "    for i in range(1, len(sequence)) :\n",
    "        if sequence[i] == result[-1][0] :\n",
    "            result[-1][1] += 1\n",
    "        else :\n",
    "            result.append([sequence[i], 1])\n",
    "    return dict(result)\n",
    "\n",
    "def structureToVASP(structure, file='POSCAR', str_name='structure', relaxed=False) :\n",
    "    with open(file + ('_relaxed' if relaxed else ''), 'w') as f :\n",
    "        f.write(str_name + '\\n')\n",
    "        f.write(str(1.0) + '\\n')\n",
    "        for axis in np.array(structure.cell[0]) :\n",
    "            for i in range(3) :\n",
    "                f.write(str(axis[i]) + '   ')\n",
    "                if i == 2 :\n",
    "                    f.write('\\n')\n",
    "        atoms = getAtomSequence(np.array(structure.atomic_numbers, dtype=int))\n",
    "        for k in atoms.keys() :\n",
    "            f.write('   ' + element(round(k)).symbol)\n",
    "        f.write('\\n')\n",
    "        for v in atoms.values() :\n",
    "            f.write('   ' + str(round(v)))\n",
    "        f.write('\\n')        \n",
    "        f.write('Cartesian\\n')\n",
    "        for position in np.array(structure.pos if not relaxed else structure.pos_relaxed)   :\n",
    "            for i in range(3) :\n",
    "                f.write(str(position[i]) + '   ')\n",
    "                if i == 2 :\n",
    "                    f.write('\\n')            \n",
    "    return None\n",
    "\n",
    "def getTranslations (positions, cell) :\n",
    "    result = np.array(positions)\n",
    "    #print(result.shape)\n",
    "    for i in range(-1,2) :\n",
    "        for j in range(-1,2) :\n",
    "            for k in range(-1,2) :\n",
    "                if (i == 0) and (j == 0) and (k == 0) :\n",
    "                    continue\n",
    "                result = np.vstack((result, np.array(positions) + \n",
    "                                    i*np.array(cell[0][0]) + \n",
    "                                    j*np.array(cell[0][1]) +\n",
    "                                    k*np.array(cell[0][2])) )\n",
    "    return result\n",
    "\n",
    "def getOffsets (positions) :\n",
    "    result = np.zeros_like(positions, dtype=int)\n",
    "    #print(result.shape)\n",
    "    for i in range(-1,2) :\n",
    "        for j in range(-1,2) :\n",
    "            for k in range(-1,2) :\n",
    "                if (i == 0) and (j == 0) and (k == 0) :\n",
    "                    continue\n",
    "                result = np.vstack((result, np.zeros_like(positions, dtype=int) + [i,j,k]) )\n",
    "    return result\n",
    "\n",
    "import functions, polyhedron, graph, geometry\n",
    "from scipy.spatial import Voronoi\n",
    "from polyhedron import Polyhedron\n",
    "import geometry as gm\n",
    "import numpy as np\n",
    "\n",
    " \n",
    "class Voro:\n",
    "\n",
    "    def __init__(self, points, central_ps, labels):\n",
    "        \"\"\"\n",
    "        :param points: list of points coordinates, [[float, float, float], ...]\n",
    "        :param central_ps: indexes of centrals points, list of int\n",
    "        :param labels: list of labels\n",
    "        \"\"\"\n",
    "        self.points = np.array(points)\n",
    "        self.central_ps = central_ps\n",
    "        self.labels = labels\n",
    "        self.vor = Voronoi(points)\n",
    "        self.p_adjacency = self.calc_p_adjacency()\n",
    "        self.polyhedrons = self.construct_polyhedrons()\n",
    "        self.rsds = np.array([(3 * p.volume / (4 * np.pi)) ** (1 / 3.) for p in self.polyhedrons])\n",
    "        self.angles = self.calc_angles()\n",
    "        self.direct_neighbors = self.find_direct_neighbors()\n",
    "\n",
    "    def calc_p_adjacency(self):\n",
    "        \"\"\"\n",
    "        Calculation of points adjacency list (points are adjacent if the domains of the points are adjacent)\n",
    "        :return: self.p_adjacency: points adjacency list, [[int, ...], ...]\n",
    "        \"\"\"\n",
    "        p_adjacency = [[] for _ in range(len(self.points))]\n",
    "        for p1, p2 in self.vor.ridge_dict.keys():\n",
    "            p_adjacency[p1] += [p2]\n",
    "            p_adjacency[p2] += [p1]\n",
    "        self.p_adjacency = p_adjacency\n",
    "        return self.p_adjacency\n",
    "\n",
    "    def construct_polyhedrons(self):\n",
    "        \"\"\"\n",
    "        Construct of the polyhedrons\n",
    "        :return: self.polyhedrons, Voronoi polyhedra, list of Polyhedron objects\n",
    "        \"\"\"\n",
    "        self.polyhedrons = []\n",
    "        for i in self.central_ps:\n",
    "            faces = []\n",
    "            region = self.vor.regions[self.vor.point_region[i]]\n",
    "            new_ind = {o_i: n_i for n_i, o_i in enumerate(region)}\n",
    "            if -1 in region:\n",
    "                raise RuntimeError(\"The domain for \\\"\" + str(i) + \"\\\" point is not closed!\")\n",
    "            for j in self.p_adjacency[i]:\n",
    "                common_vs = self.vor.ridge_dict.get((i, j))\n",
    "                if common_vs is None:\n",
    "                    common_vs = self.vor.ridge_dict[(j, i)][::-1]\n",
    "                if i != j and common_vs is not None and len(common_vs) > 2:\n",
    "                    faces += [[new_ind[o_i] for o_i in common_vs]]\n",
    "                else:\n",
    "                    raise RuntimeError(\"The Voronoi decomposition is failed!\")\n",
    "            faces = np.array([np.array(f) for f in faces])\n",
    "            self.polyhedrons += [Polyhedron(self.vor.vertices[region], region, faces, find_order=False)]\n",
    "        return self.polyhedrons\n",
    "\n",
    "    def calc_angles(self):\n",
    "        \"\"\"\n",
    "        Calculation of solid angles\n",
    "        :return: self.angles, solid angles between adjacent Voronoi polyhedra, {(int, int): float, ...}\n",
    "        \"\"\"\n",
    "        self.angles = {}\n",
    "        for i, i1 in enumerate(self.central_ps):\n",
    "            angles = []\n",
    "            cp = self.points[i1]\n",
    "            for j, i2 in enumerate(self.p_adjacency[i1]):\n",
    "                angles += [sum([abs(gm.calc_solid_angle(cp, s)) for s in self.polyhedrons[i].faces[j].simplexes])]\n",
    "            angles = 100 * np.array(angles) / sum(angles)\n",
    "            for j, i2 in enumerate(self.p_adjacency[i1]):\n",
    "                self.angles[(i1, i2)] = angles[j]\n",
    "                self.angles[(i2, i1)] = angles[j]\n",
    "        return self.angles\n",
    "\n",
    "    def find_direct_neighbors(self):\n",
    "        \"\"\"\n",
    "        Find the direct neighbors\n",
    "        :return: self.direct_neighbors: direct Voronoi polyhedra neighbors, {(int, int): bool}\n",
    "        \"\"\"\n",
    "        self.direct_neighbors = {}\n",
    "        for i, i1 in enumerate(self.central_ps):\n",
    "            p1 = self.points[i1]\n",
    "            for j, i2 in enumerate(self.p_adjacency[i1]):\n",
    "                p2 = self.points[i2]\n",
    "                if self.polyhedrons[i].faces[j].is_inside(gm.calc_centroid([p1, p2])):\n",
    "                    self.direct_neighbors[(i1, i2)] = True\n",
    "                    self.direct_neighbors[(i2, i1)] = True\n",
    "        return self.direct_neighbors\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import lmdb\n",
    "import torch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89b96ec5-3795-4d15-9854-72cc2c345458",
   "metadata": {},
   "source": [
    "import torch\n",
    "import lmdb\n",
    "dataset = SinglePointLmdbDataset({\"src\": '/Users/Eremin/OneDrive/Share/10k/train/data.lmdb'})\n",
    "print(len(dataset))\n",
    "for relaxed in [True, False] :\n",
    "    structureToVASP(dataset[0], relaxed=relaxed)\n",
    "dataset[0].keys\n",
    "for prop in ['cell_offsets', 'edge_index', 'atomic_numbers', 'distances']:\n",
    "    print(prop, dataset[0][prop].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b8e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def newProperties(data) :\n",
    "    keys = []\n",
    "    result = []\n",
    "    \n",
    "    points = getTranslations(data.pos, data.cell)\n",
    "    atom_index = np.array(list(range(data.natoms)) * round(len(points) / len(data.pos)))\n",
    "    offsets = getOffsets(data.pos)\n",
    "    voro = Voro(points, range(data.natoms), list(range(len(points))))\n",
    "    \n",
    "    volumes = np.array(list(map(lambda x: x.volume, voro.polyhedrons)))\n",
    "    surface_areas = np.array(list(map(lambda x: x.area, voro.polyhedrons)))\n",
    "    rsds = voro.rsds\n",
    "    \n",
    "    keys.extend(['voronoi_volumes', 'voronoi_surface_areas',\n",
    "                'spherical_domain_radii'])\n",
    "    result.extend([volumes, surface_areas, rsds])    \n",
    "    \n",
    "    df = pd.DataFrame(voro.angles.keys(), columns=['VA_p1', 'VA_p2'])\n",
    "    #print(df)\n",
    "    df['cell_offsets'] = list(map(tuple, offsets[df['VA_p1']] - offsets[df['VA_p2']])) \n",
    "    #sign correspond to original data\n",
    "    df['VA_p1_corr'] = atom_index[df['VA_p1']]\n",
    "    df['VA_p2_corr'] = atom_index[df['VA_p2']]\n",
    "    df['direct_neighbor'] = list(map(lambda x: 1 if x in voro.direct_neighbors.keys() else 0, voro.angles.keys()))\n",
    "    k = np.array(list(voro.angles.keys()))\n",
    "    df['distance'] = np.array([euclidean(points[pair[0]], points[pair[1]]) for pair in k])\n",
    "    df['solid_angle'] = voro.angles.values()\n",
    "    df['to_keep'] = ~df.duplicated(subset=['VA_p1_corr', 'VA_p2_corr', 'cell_offsets'], keep='first')\n",
    "    df = df[df['to_keep']].drop(labels=['to_keep', 'VA_p1', 'VA_p2'], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    keys.extend(['cell_offsets_new', 'distances_new', 'contact_solid_angles',\n",
    "                'direct_neighbor', 'edge_index_new'])\n",
    "    result.extend([np.array(list(map(np.array, df['cell_offsets'].values))), df['distance'].values, \n",
    "                   df['solid_angle'].values, df['direct_neighbor'].values, \n",
    "                   df[['VA_p1_corr', 'VA_p2_corr']].values.T])  \n",
    "    \n",
    "    return dict(zip(keys, result))\n",
    "\n",
    "def processDataset (dataset_path, dataset_batch_size=5004) :\n",
    "    dataset_path_modified = re.sub('\\.lmdb', '_mod.lmdb', dataset_path)\n",
    "    print(dataset_path_modified)\n",
    "    print()\n",
    "    \n",
    "    dataset = SinglePointLmdbDataset({\"src\": dataset_path})\n",
    "    print('Original dataset size is {}'.format(len(dataset)))\n",
    "    print()\n",
    "    \n",
    "    dataset_target = lmdb.open(\n",
    "        dataset_path_modified,\n",
    "        map_size=int(1e9*200), #~ 200 Gbyte\n",
    "        subdir=False,\n",
    "        meminit=False,\n",
    "        map_async=True,)\n",
    "    \n",
    "    print('Batch info:')\n",
    "    \n",
    "    for batch in range(len(dataset) // dataset_batch_size + \n",
    "                       (1 if len(dataset) % dataset_batch_size != 0 else 0)) :\n",
    "        print('{}: from {} to {}'.format(batch, batch * dataset_batch_size, \n",
    "                                         min(len(dataset), (batch+1) * dataset_batch_size) - 1))\n",
    "    \n",
    "    print()\n",
    "    for batch in range(len(dataset) // dataset_batch_size + \n",
    "                       (1 if len(dataset) % dataset_batch_size != 0 else 0)) :\n",
    "        \n",
    "        dataset_under_process = [dataset[i] for i in \n",
    "                                 range(batch * dataset_batch_size, \n",
    "                                       min(len(dataset), (batch+1) * dataset_batch_size))]\n",
    "        print('Structures from {} to {} are under process...'.format(batch * dataset_batch_size, \n",
    "                                                batch * dataset_batch_size + len(dataset_under_process) - 1))\n",
    "        \n",
    "        res = Parallel(n_jobs=-1)(delayed(newProperties)(dataset_under_process[i]) \n",
    "                                  for i in tqdm(range(len(dataset_under_process))))\n",
    "        #res = [dict() for i in range(len(dataset_under_process))]\n",
    "        print('are stored to a file...')\n",
    "        \n",
    "        for structure_id in tqdm(range(len(dataset_under_process))) :\n",
    "            txn = dataset_target.begin(write=True)\n",
    "            data = dataset_under_process[structure_id]\n",
    "            \n",
    "            for new_data_key in res[0].keys() :\n",
    "                #print(new_data_key)\n",
    "                data[new_data_key] = torch.from_numpy(res[structure_id][new_data_key])\n",
    "                #print(data[new_data_key])\n",
    "            txn.put(f\"{structure_id + batch * dataset_batch_size}\".encode(\"ascii\"), \n",
    "                    pickle.dumps(data, protocol=-1))\n",
    "            txn.commit()\n",
    "            dataset_target.sync()\n",
    "    \n",
    "    dataset_target.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e7d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processDataset('/Users/Eremin/OneDrive/Share/10k/train/data.lmdb')\n",
    "processDataset('../../../ocp_datasets/data/is2re/100k/train/data.lmdb')\n",
    "processDataset('../../../ocp_datasets/data/is2re/all/train/data.lmdb')\n",
    "\n",
    "processDataset('../../../ocp_datasets/data/is2re/all/val_ood_cat/data.lmdb')\n",
    "processDataset('../../../ocp_datasets/data/is2re/all/val_ood_ads/data.lmdb')\n",
    "processDataset('../../../ocp_datasets/data/is2re/all/val_ood_both/data.lmdb')\n",
    "processDataset('../../../ocp_datasets/data/is2re/all/test_ood_cat/data.lmdb')\n",
    "processDataset('../../../ocp_datasets/data/is2re/all/val_ood_cat/data.lmdb')\n",
    "\n",
    "\n",
    "# processDataset('../../ocp_datasets/data/is2re/all/test_ood_both/data.lmdb')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "133e3236-d4d0-4506-ad31-7063786018c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "def newProperties(data, i) :\n",
    "    keys = ['id', 'data']\n",
    "    result = [i, data]\n",
    "    \n",
    "    #### remove atom embeddings\n",
    "    #atomic_numbers = np.array(data.atomic_numbers, dtype=int)\n",
    "    #unique_atoms = np.unique(atomic_numbers).tolist()\n",
    "    #atomic_info = element(unique_atoms)\n",
    "\n",
    "    #prop1 = dict(zip(unique_atoms, map(lambda x: x.covalent_radius_cordero / 100, atomic_info)))\n",
    "    #prop2 = dict(zip(unique_atoms, map(lambda x: x.dipole_polarizability, atomic_info)))\n",
    "    #prop3 = dict(zip(unique_atoms, map(lambda x: x.electron_affinity, atomic_info)))\n",
    "    #prop4 = dict(zip(unique_atoms, map(lambda x: x.en_pauling, atomic_info)))\n",
    "\n",
    "    #covalent_radii = np.array([prop1[i] for i in atomic_numbers])\n",
    "    #dipole_polarizability = np.array([prop2[i] for i in atomic_numbers])\n",
    "    #electron_affinity = np.array([prop3[i] for i in atomic_numbers])\n",
    "    #electronegativity = np.array([prop4[i] for i in atomic_numbers])\n",
    "    \n",
    "    #keys.extend(['covalent_radii', 'dipole_polarizability',\n",
    "     #           'electron_affinity', 'electronegativity'])\n",
    "    #result.extend([covalent_radii, dipole_polarizability,\n",
    "     #           electron_affinity, electronegativity])\n",
    "    \n",
    "    points = getTranslations(data.pos, data.cell)\n",
    "    atom_index = np.array(list(range(data.natoms)) * round(len(points) / len(data.pos)))\n",
    "    offsets = getOffsets(data.pos)\n",
    "    voro = Voro(points, range(data.natoms), list(range(len(points))))\n",
    "    \n",
    "    volumes = np.array(list(map(lambda x: x.volume, voro.polyhedrons)))\n",
    "    surface_areas = np.array(list(map(lambda x: x.area, voro.polyhedrons)))\n",
    "    rsds = voro.rsds\n",
    "    \n",
    "    keys.extend(['voronoi_volumes', 'voronoi_surface_areas',\n",
    "                'spherical_domain_radii'])\n",
    "    result.extend([volumes, surface_areas, rsds])    \n",
    "    \n",
    "    #fixed incorrect edge indexing\n",
    "\n",
    "    df = pd.DataFrame(voro.angles.keys(), columns=['VA_p1', 'VA_p2'])\n",
    "    #print(df)\n",
    "    df['cell_offsets'] = list(map(tuple, offsets[df['VA_p1']] - offsets[df['VA_p2']])) #sign correspond to original data\n",
    "    df['VA_p1_corr'] = atom_index[df['VA_p1']]\n",
    "    df['VA_p2_corr'] = atom_index[df['VA_p2']]\n",
    "    df['direct_neighbor'] = list(map(lambda x: 1 if x in voro.direct_neighbors.keys() else 0, voro.angles.keys()))\n",
    "    k = np.array(list(voro.angles.keys()))\n",
    "    df['distance'] = np.array([euclidean(points[pair[0]], points[pair[1]]) for pair in k])\n",
    "    df['solid_angle'] = voro.angles.values()\n",
    "    df['to_keep'] = ~df.duplicated(subset=['VA_p1_corr', 'VA_p2_corr', 'cell_offsets'], keep='first')\n",
    "    df = df[df['to_keep']].drop(labels=['to_keep', 'VA_p1', 'VA_p2'], axis=1).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    keys.extend(['cell_offsets_new', 'distances_new', 'contact_solid_angles',\n",
    "                'direct_neighbor', 'edge_index_new'])\n",
    "    result.extend([df['cell_offsets'].values, df['distance'].values, \n",
    "                   df['solid_angle'].values, df['direct_neighbor'].values, \n",
    "                   df[['VA_p1_corr', 'VA_p2_corr']].values.T])  \n",
    "    #end\n",
    "    \n",
    "    return dict(zip(keys, result))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1d6c241",
   "metadata": {
    "tags": []
   },
   "source": [
    "#test for the 1st structure\n",
    "newProperties(dataset[0], 0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8fc215bc-8a73-4bb8-a025-dd5c5411ed1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#for directory in [''] :\n",
    "for directory in ['../../all/val_ood_both/' , '../../all/test_ood_both/'] :\n",
    "    print(directory)\n",
    "    dataset = SinglePointLmdbDataset({\"src\": directory+'data.lmdb'})\n",
    "    res = Parallel(n_jobs=-1)(delayed(newProperties)(dataset[i], i) for i in tqdm(range(len(dataset))))\n",
    "    with open(directory+'structures.pkl', 'wb') as f :\n",
    "        pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d55932fd",
   "metadata": {},
   "source": [
    "with open('structures.pkl', 'rb') as f :\n",
    "    data = pickle.load(f)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d85c50f-026f-4e28-bb71-576ff7f25ec9",
   "metadata": {
    "tags": []
   },
   "source": [
    "for directory in ['../../100k/train/'] :\n",
    "    print(directory)\n",
    "    dataset = SinglePointLmdbDataset({\"src\": directory+'data.lmdb'})\n",
    "    res = Parallel(n_jobs=-1)(delayed(newProperties)(dataset[i], i) for i in tqdm(range(len(dataset))))\n",
    "    with open(directory+'structures.pkl', 'wb') as f :\n",
    "        pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b71711e9-108b-4544-a0de-0589e5f3f608",
   "metadata": {},
   "source": [
    "for directory in ['../../all/val_ood_cat/' , '../../all/test_ood_cat/',\n",
    "                  '../../all/val_ood_ads/' , '../../all/test_ood_ads/',\n",
    "                  '../../all/val_id/' , '../../all/test_id/'] :\n",
    "    print(directory)\n",
    "    dataset = SinglePointLmdbDataset({\"src\": directory+'data.lmdb'})\n",
    "    res = Parallel(n_jobs=-1)(delayed(newProperties)(dataset[i], i) for i in tqdm(range(len(dataset))))\n",
    "    with open(directory+'structures.pkl', 'wb') as f :\n",
    "        pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a49ded8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
