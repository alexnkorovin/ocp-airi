{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SchNet S2EF training example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to demonstrate some of the basics of the Open Catalyst Project's (OCP) codebase and data. In this example, we will train a schnet model for predicting the energy and forces of a given structure (S2EF task). First, ensure you have installed the OCP ocp repo and all the dependencies according to the [README](https://github.com/Open-Catalyst-Project/ocp/blob/master/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: This notebook is for tutorial purposes, it is unlikely it will be practical to train baseline models on our larger datasets using this format. As a next step, we recommend trying the command line examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from ocpmodels.trainers import ForcesTrainer\n",
    "from ocpmodels import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# a simple sanity check that a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The essential steps for training an OCP model\n",
    "\n",
    "1) Download data\n",
    "\n",
    "2) Preprocess data (if necessary)\n",
    "\n",
    "3) Define or load a configuration (config), which includes the following\n",
    "   \n",
    "   - task\n",
    "   - model\n",
    "   - optimizer\n",
    "   - dataset\n",
    "   - trainer\n",
    "\n",
    "4) Train\n",
    "\n",
    "5) Depending on the model/task there might be intermediate relaxation step\n",
    "\n",
    "6) Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This examples uses the LMDB generated from the following [tutorial](http://laikapack.cheme.cmu.edu/notebook/open-catalyst-project/mshuaibi/notebooks/projects/ocp/docs/source/tutorials/lmdb_dataset_creation.ipynb). Please run that notebook before moving on. Alternatively, if you have other LMDBs available you may specify that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to your local lmdb directory\n",
    "train_src = \"s2ef\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will explicitly define the config; however, a set of default config files exists in the config folder of this repository. Default config yaml files can easily be loaded with the `build_config` util (found in `ocp/ocpmodels/common/utils.py`). Loading a yaml config is preferrable when launching jobs from the command line. We have included our best models' config files [here](https://github.com/Open-Catalyst-Project/ocp/tree/master/configs/s2ef)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = {\n",
    "    'dataset': 'trajectory_lmdb', # dataset used for the S2EF task\n",
    "    'description': 'Regressing to energies and forces for DFT trajectories from OCP',\n",
    "    'type': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'labels': ['potential energy'],\n",
    "    'grad_input': 'atomic forces',\n",
    "    'train_on_free_atoms': True,\n",
    "    'eval_on_free_atoms': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model** - SchNet for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    'name': 'schnet',\n",
    "    'hidden_channels': 1024, # if training is too slow for example purposes reduce the number of hidden channels\n",
    "    'num_filters': 256,\n",
    "    'num_interactions': 3,\n",
    "    'num_gaussians': 200,\n",
    "    'cutoff': 6.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = {\n",
    "    'batch_size': 16, # if hitting GPU memory issues, lower this\n",
    "    'eval_batch_size': 8,\n",
    "    'num_workers': 8,\n",
    "    'lr_initial': 0.0001,\n",
    "    'lr_gamma': 0.1,\n",
    "    'lr_milestones': [15, 20],\n",
    "    'warmup_epochs': 10,\n",
    "    'warmup_factor': 0.2,\n",
    "    'force_coefficient': 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, `train_src` is used for all the train/val/test sets. Feel free to update with the actual S2EF val and test sets, but it does require additional downloads and preprocessing. If you desire to normalize your targets, `normalize_labels` must be set to `True` and corresponding `mean` and `stds` need to be specified. These values have been precomputed for you and can be found in any of the [`base.yml`](https://github.com/Open-Catalyst-Project/ocp/blob/master/configs/s2ef/20M/base.yml#L5-L9) config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "{'src': train_src, 'normalize_labels': False}, # train set \n",
    "{'src': train_src}, # val set (optional)\n",
    "{'src': train_src} # test set (optional - writes predictions to disk)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trainer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `ForcesTrainer` for the S2EF and IS2RS tasks, and the `EnergyTrainer` for the IS2RE task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amp: false\n",
      "cmd:\n",
      "  checkpoint_dir: ./checkpoints/2021-08-04-18-49-20-SchNet-example\n",
      "  commit: '3546713'\n",
      "  identifier: SchNet-example\n",
      "  logs_dir: ./logs/tensorboard/2021-08-04-18-49-20-SchNet-example\n",
      "  print_every: 10\n",
      "  results_dir: ./results/2021-08-04-18-49-20-SchNet-example\n",
      "  seed: 0\n",
      "  timestamp: 2021-08-04-18-49-20-SchNet-example\n",
      "dataset:\n",
      "  normalize_labels: false\n",
      "  src: s2ef\n",
      "gpus: 0\n",
      "logger: tensorboard\n",
      "model: schnet\n",
      "model_attributes:\n",
      "  cutoff: 6.0\n",
      "  hidden_channels: 1024\n",
      "  num_filters: 256\n",
      "  num_gaussians: 200\n",
      "  num_interactions: 3\n",
      "optim:\n",
      "  batch_size: 16\n",
      "  eval_batch_size: 8\n",
      "  force_coefficient: 100\n",
      "  lr_gamma: 0.1\n",
      "  lr_initial: 0.0001\n",
      "  lr_milestones:\n",
      "  - 15\n",
      "  - 20\n",
      "  num_workers: 8\n",
      "  warmup_epochs: 10\n",
      "  warmup_factor: 0.2\n",
      "task:\n",
      "  dataset: trajectory_lmdb\n",
      "  description: Regressing to energies and forces for DFT trajectories from OCP\n",
      "  eval_on_free_atoms: true\n",
      "  grad_input: atomic forces\n",
      "  labels:\n",
      "  - potential energy\n",
      "  metric: mae\n",
      "  train_on_free_atoms: true\n",
      "  type: regression\n",
      "test_dataset:\n",
      "  src: s2ef\n",
      "val_dataset:\n",
      "  src: s2ef\n",
      "\n",
      "### Loading dataset: trajectory_lmdb\n",
      "### Loading model: schnet\n",
      "### Loaded SchNet with 5704193 parameters.\n",
      "NOTE: model gradient logging to tensorboard not yet supported.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "ConfigError: please define lr_milestones in steps not epochs and define warmup_steps instead of warmup_epochs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r0/sh_xx3ms6_j0__w1_7f87p540000gq/T/ipykernel_7792/4280866787.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainer = ForcesTrainer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ocp/ocpmodels/trainers/forces_trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, task, model, dataset, optimizer, identifier, run_dir, is_debug, is_vis, is_hpo, print_every, seed, logger, local_rank, amp, cpu)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mcpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     ):\n\u001b[0;32m---> 83\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ocp/ocpmodels/trainers/base_trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, task, model, dataset, optimizer, identifier, run_dir, is_debug, is_vis, is_hpo, print_every, seed, logger, local_rank, amp, cpu, name)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_master\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_flow_style\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ocp/ocpmodels/trainers/base_trainer.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_extras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;31m# Note: this function is now deprecated. We build config outside of trainer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ocp/ocpmodels/trainers/base_trainer.py\u001b[0m in \u001b[0;36mload_extras\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_extras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;31m# metrics.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ocp/ocpmodels/modules/scheduler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, optimizer, config)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mscheduler_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mscheduler_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ocp-models/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, optimizer, lr_lambda, last_epoch, verbose)\u001b[0m\n\u001b[1;32m    201\u001b[0m                     len(optimizer.param_groups), len(lr_lambda)))\n\u001b[1;32m    202\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_lambdas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_lambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLambdaLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ocp-models/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, optimizer, last_epoch, verbose)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ocp-models/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH_DEPRECATION_WARNING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUserWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ocp-models/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mget_lr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m                           \"please use `get_last_lr()`.\")\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         return [base_lr * lmbda(self.last_epoch)\n\u001b[0m\u001b[1;32m    251\u001b[0m                 for lmbda, base_lr in zip(self.lr_lambdas, self.base_lrs)]\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/ocp-models/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    248\u001b[0m                           \"please use `get_last_lr()`.\")\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         return [base_lr * lmbda(self.last_epoch)\n\u001b[0m\u001b[1;32m    251\u001b[0m                 for lmbda, base_lr in zip(self.lr_lambdas, self.base_lrs)]\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ocp/ocpmodels/modules/scheduler.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LambdaLR\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mscheduler_lambda_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwarmup_lr_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr_lambda\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscheduler_lambda_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ocp/ocpmodels/common/utils.py\u001b[0m in \u001b[0;36mwarmup_lr_lambda\u001b[0;34m(current_step, optim_config)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mor\u001b[0m \u001b[0;34m\"warmup_epochs\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptim_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     ):\n\u001b[0;32m---> 73\u001b[0;31m         raise Exception(\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;34m\"ConfigError: please define lr_milestones in steps not epochs and define warmup_steps instead of warmup_epochs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         )\n",
      "\u001b[0;31mException\u001b[0m: ConfigError: please define lr_milestones in steps not epochs and define warmup_steps instead of warmup_epochs"
     ]
    }
   ],
   "source": [
    "trainer = ForcesTrainer(\n",
    "    task=task,\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    optimizer=optimizer,\n",
    "    identifier=\"SchNet-example\",\n",
    "    run_dir=\"./\", # directory to save results if is_debug=False. Prediction files are saved here so be careful not to override!\n",
    "    is_debug=False, # if True, do not save checkpoint, logs, or results\n",
    "    is_vis=False,\n",
    "    print_every=10,\n",
    "    seed=0, # random seed to use\n",
    "    logger=\"tensorboard\", # logger of choice (tensorboard and wandb supported)\n",
    "    local_rank=0,\n",
    "    amp=False, # use PyTorch Automatic Mixed Precision (faster training and less memory usage)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r0/sh_xx3ms6_j0__w1_7f87p540000gq/T/ipykernel_7792/2274016240.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "print(trainer.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r0/sh_xx3ms6_j0__w1_7f87p540000gq/T/ipykernel_7792/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Checkpoint\n",
    "Once training has completed a `Trainer` class, by default, is loaded with the best checkpoint as determined by training or validation (if available) metrics. To load a `Trainer` class directly with a pretrained model, specify the `checkpoint_path` as defined by your previously trained model (`checkpoint_dir`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./checkpoints/2021-03-06-15-47-12-SchNet-example/checkpoint.pt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = os.path.join(trainer.config[\"cmd\"][\"checkpoint_dir\"], \"checkpoint.pt\")\n",
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amp: false\n",
      "cmd:\n",
      "  checkpoint_dir: ./checkpoints/2021-03-06-15-51-28-SchNet-example\n",
      "  commit: 046e2fa\n",
      "  identifier: SchNet-example\n",
      "  logs_dir: ./logs/tensorboard/2021-03-06-15-51-28-SchNet-example\n",
      "  print_every: 10\n",
      "  results_dir: ./results/2021-03-06-15-51-28-SchNet-example\n",
      "  seed: 0\n",
      "  timestamp: 2021-03-06-15-51-28-SchNet-example\n",
      "dataset:\n",
      "  normalize_labels: false\n",
      "  src: s2ef\n",
      "gpus: 1\n",
      "logger: tensorboard\n",
      "logger_entity: null\n",
      "logger_project: null\n",
      "model: schnet\n",
      "model_attributes:\n",
      "  cutoff: 6.0\n",
      "  hidden_channels: 1024\n",
      "  num_filters: 256\n",
      "  num_gaussians: 200\n",
      "  num_interactions: 3\n",
      "optim:\n",
      "  batch_size: 16\n",
      "  eval_batch_size: 8\n",
      "  force_coefficient: 100\n",
      "  lr_gamma: 0.1\n",
      "  lr_initial: 0.0001\n",
      "  lr_milestones:\n",
      "  - 15\n",
      "  - 20\n",
      "  max_epochs: 1\n",
      "  num_workers: 8\n",
      "  warmup_epochs: 10\n",
      "  warmup_factor: 0.2\n",
      "task:\n",
      "  dataset: trajectory_lmdb\n",
      "  description: Regressing to energies and forces for DFT trajectories from OCP\n",
      "  eval_on_free_atoms: true\n",
      "  grad_input: atomic forces\n",
      "  labels:\n",
      "  - potential energy\n",
      "  metric: mae\n",
      "  train_on_free_atoms: true\n",
      "  type: regression\n",
      "test_dataset:\n",
      "  src: s2ef\n",
      "val_dataset:\n",
      "  src: s2ef\n",
      "\n",
      "### Loading dataset: trajectory_lmdb\n",
      "### Loading model: schnet\n",
      "### Loaded SchNet with 5704193 parameters.\n",
      "NOTE: model gradient logging to tensorboard not yet supported.\n",
      "### Loading checkpoint from: ./checkpoints/2021-03-06-15-47-12-SchNet-example/checkpoint.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/miniconda3/envs/ocp-models/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = {\n",
    "    'name': 'schnet',\n",
    "    'hidden_channels': 1024, # if training is too slow for example purposes reduce the number of hidden channels\n",
    "    'num_filters': 256,\n",
    "    'num_interactions': 3,\n",
    "    'num_gaussians': 200,\n",
    "    'cutoff': 6.0\n",
    "}\n",
    "\n",
    "pretrained_trainer = ForcesTrainer(\n",
    "    task=task,\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    optimizer=optimizer,\n",
    "    identifier=\"SchNet-example\",\n",
    "    run_dir=\"./\", # directory to save results if is_debug=False. Prediction files are saved here so be careful not to override!\n",
    "    is_debug=False, # if True, do not save checkpoint, logs, or results\n",
    "    is_vis=False,\n",
    "    print_every=10,\n",
    "    seed=0, # random seed to use\n",
    "    logger=\"tensorboard\", # logger of choice (tensorboard and wandb supported)\n",
    "    local_rank=0,\n",
    "    amp=False, # use PyTorch Automatic Mixed Precision (faster training and less memory usage)\n",
    ")\n",
    "\n",
    "pretrained_trainer.load_pretrained(checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a test has been provided in your config, predictions are generated and written to disk automatically upon training completion. Otherwise, to make predictions on unseen data a `torch.utils.data` DataLoader object must be constructed. Here we reference our test set to make predictions on. Predictions are saved in `{results_file}.npz` in your `results_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Predicting on test.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device 0: 100%|██████████| 126/126 [00:01<00:00, 93.22it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to ./results/2021-03-06-15-51-28-SchNet-example/s2ef_s2ef_results.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions on the existing test_loader\n",
    "predictions = pretrained_trainer.predict(pretrained_trainer.test_loader, results_file=\"s2ef_results\", disable_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = predictions[\"energy\"]\n",
    "forces = predictions[\"forces\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocp-models",
   "language": "python",
   "name": "ocp-models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
